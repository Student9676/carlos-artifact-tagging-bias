import pandas as pd
from sklearn.metrics import f1_score

bert_results = pd.read_excel('C:/Users/LC180/Desktop/bert_v2_2_test_data.xlsx')
my_model_results = pd.read_excel('C:/Users/LC180/Desktop/processed_output 下午11.26.49.xlsx')
my_model_results[['Subjective', 'Gender', 'Jargon', 'Social']] = my_model_results['Binary_Result'].str.split(' - ', expand=True).astype(int)
bert_results['Binary_Result'] = bert_results.apply(
    lambda row: f"{row['Subjective']} - {row['Gender']} - {row['Jargon']} - {row['Social']}", axis=1
)


if len(bert_results) == len(my_model_results):
    # Extract individual columns for comparison
    bert_subjective = bert_results['Subjective']
    bert_gender = bert_results['Gender']
    bert_jargon = bert_results['Jargon']
    bert_social = bert_results['Social']

    model_subjective = my_model_results['Subjective']
    model_gender = my_model_results['Gender']
    model_jargon = my_model_results['Jargon']
    model_social = my_model_results['Social']

    # Concatenate all predictions and ground truths
    y_true = pd.concat([bert_subjective, bert_gender, bert_jargon, bert_social], axis=0)
    y_pred = pd.concat([model_subjective, model_gender, model_jargon, model_social], axis=0)

    # Calculate overall F1 scores
    f1_macro = f1_score(y_true, y_pred, average='macro')
    f1_micro = f1_score(y_true, y_pred, average='micro')
    f1_weighted = f1_score(y_true, y_pred, average='weighted')

    # Print overall F1 scores
    print(f"Overall F1 Score (Macro): {f1_macro:.4f}")
    print(f"Overall F1 Score (Micro): {f1_micro:.4f}")
    print(f"Overall F1 Score (Weighted): {f1_weighted:.4f}")

