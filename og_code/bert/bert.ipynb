{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2.2 concatenates all texts by only keeping sentences where bias is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.3 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: xlsxwriter==3.2.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: regex==2024.9.11 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 3)) (2024.9.11)\n",
      "Requirement already satisfied: openpyxl==3.1.5 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 4)) (3.1.5)\n",
      "Requirement already satisfied: pysbd==0.3.4 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 5)) (0.3.4)\n",
      "Requirement already satisfied: transformers==4.45.2 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 6)) (4.45.2)\n",
      "Requirement already satisfied: torch==2.5.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 7)) (2.5.0)\n",
      "Requirement already satisfied: scikit-learn==1.5.2 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 8)) (1.5.2)\n",
      "Requirement already satisfied: datasets==3.0.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 9)) (3.0.1)\n",
      "Requirement already satisfied: seaborn==0.13.2 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 10)) (0.13.2)\n",
      "Requirement already satisfied: accelerate==1.0.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 11)) (1.0.0)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 12)) (6.29.5)\n",
      "Requirement already satisfied: torchvision==0.20.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 13)) (0.20.0)\n",
      "Requirement already satisfied: torchaudio==2.5.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 14)) (2.5.0)\n",
      "Requirement already satisfied: tensorboardX==2.6.2.2 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from -r ../requirements.txt (line 15)) (2.6.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from pandas==2.2.3->-r ../requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from pandas==2.2.3->-r ../requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from pandas==2.2.3->-r ../requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from pandas==2.2.3->-r ../requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: et-xmlfile in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from openpyxl==3.1.5->-r ../requirements.txt (line 4)) (2.0.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from transformers==4.45.2->-r ../requirements.txt (line 6)) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from transformers==4.45.2->-r ../requirements.txt (line 6)) (0.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from transformers==4.45.2->-r ../requirements.txt (line 6)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from transformers==4.45.2->-r ../requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from transformers==4.45.2->-r ../requirements.txt (line 6)) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from transformers==4.45.2->-r ../requirements.txt (line 6)) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from transformers==4.45.2->-r ../requirements.txt (line 6)) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from transformers==4.45.2->-r ../requirements.txt (line 6)) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from torch==2.5.0->-r ../requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from torch==2.5.0->-r ../requirements.txt (line 7)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from torch==2.5.0->-r ../requirements.txt (line 7)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from torch==2.5.0->-r ../requirements.txt (line 7)) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from torch==2.5.0->-r ../requirements.txt (line 7)) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from torch==2.5.0->-r ../requirements.txt (line 7)) (1.13.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from scikit-learn==1.5.2->-r ../requirements.txt (line 8)) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from scikit-learn==1.5.2->-r ../requirements.txt (line 8)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from scikit-learn==1.5.2->-r ../requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from datasets==3.0.1->-r ../requirements.txt (line 9)) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from datasets==3.0.1->-r ../requirements.txt (line 9)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from datasets==3.0.1->-r ../requirements.txt (line 9)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from datasets==3.0.1->-r ../requirements.txt (line 9)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from datasets==3.0.1->-r ../requirements.txt (line 9)) (3.10.10)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from seaborn==0.13.2->-r ../requirements.txt (line 10)) (3.9.2)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from accelerate==1.0.0->-r ../requirements.txt (line 11)) (6.1.0)\n",
      "Requirement already satisfied: appnope in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipykernel==6.29.5->-r ../requirements.txt (line 12)) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipykernel==6.29.5->-r ../requirements.txt (line 12)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipykernel==6.29.5->-r ../requirements.txt (line 12)) (1.8.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipykernel==6.29.5->-r ../requirements.txt (line 12)) (8.29.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipykernel==6.29.5->-r ../requirements.txt (line 12)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipykernel==6.29.5->-r ../requirements.txt (line 12)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipykernel==6.29.5->-r ../requirements.txt (line 12)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipykernel==6.29.5->-r ../requirements.txt (line 12)) (1.6.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipykernel==6.29.5->-r ../requirements.txt (line 12)) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipykernel==6.29.5->-r ../requirements.txt (line 12)) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipykernel==6.29.5->-r ../requirements.txt (line 12)) (5.14.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from torchvision==0.20.0->-r ../requirements.txt (line 13)) (11.0.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from tensorboardX==2.6.2.2->-r ../requirements.txt (line 15)) (5.29.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.5.0->-r ../requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from aiohttp->datasets==3.0.1->-r ../requirements.txt (line 9)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from aiohttp->datasets==3.0.1->-r ../requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from aiohttp->datasets==3.0.1->-r ../requirements.txt (line 9)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from aiohttp->datasets==3.0.1->-r ../requirements.txt (line 9)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from aiohttp->datasets==3.0.1->-r ../requirements.txt (line 9)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from aiohttp->datasets==3.0.1->-r ../requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: decorator in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (4.3.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r ../requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r ../requirements.txt (line 10)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r ../requirements.txt (line 10)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r ../requirements.txt (line 10)) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn==0.13.2->-r ../requirements.txt (line 10)) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r ../requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from requests->transformers==4.45.2->-r ../requirements.txt (line 6)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from requests->transformers==4.45.2->-r ../requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from requests->transformers==4.45.2->-r ../requirements.txt (line 6)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from requests->transformers==4.45.2->-r ../requirements.txt (line 6)) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from jinja2->torch==2.5.0->-r ../requirements.txt (line 7)) (3.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (0.2.13)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.0.1->-r ../requirements.txt (line 9)) (0.2.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel==6.29.5->-r ../requirements.txt (line 12)) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING AND PREPROCESSING DATA + ANALYTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers, torch\n",
    "\n",
    "\"\"\"\n",
    "Remove uneecessay new lines\n",
    "remove duplicates within one cell\n",
    "Remove links?\n",
    "use keywords to make sure that correct classification is done\n",
    "make validation set more class balanced, i.e. equal num of examples for each category\n",
    "\"\"\"\n",
    "# use keras instead of huggung face to make it easier to work with messing with layers\n",
    "# keep latest entries only in original clean data\n",
    "# double or triple (for some biases) rows that have social, gender, or subjective bias\n",
    "# remove entries greater than 512 words to remove noise\n",
    "# enchance data by repeating key terms\n",
    "# cut 512 from middle of the dataset\n",
    "# try giving it only the labels with 5 word context\n",
    "# try doing subtext technique to give it 1000 words\n",
    "\n",
    "# load data and rename TextEntry column\n",
    "df = pd.read_csv(\"../carlos_data/preprocessed_data.csv\", encoding=\"utf-8\")\n",
    "df = df.rename(columns={\"TextEntry\":\"Description\"})\n",
    "df.drop(columns=['Subjective Label', 'Gender Label', 'Jargon Label', 'Social Label', \"ANNOTATED?\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 1442\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 180\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 181\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers, torch\n",
    "\n",
    "# convert data to dictionary\n",
    "data = df.to_dict(\"records\")\n",
    "# Split the data into train and validation and test sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "train_dict, test_dict = train_test_split(data, test_size=0.20, random_state=42) # specifying this random state allows for consistent train, val, and test sets\n",
    "test_dict, validation_dict = train_test_split(test_dict, test_size=0.50, random_state=42) # same here\n",
    "\n",
    "# Create Dataset objects\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_list(train_dict)\n",
    "test_dataset = Dataset.from_list(test_dict)\n",
    "validation_dataset = Dataset.from_list(validation_dict)\n",
    "\n",
    "# Create DatasetDict\n",
    "from datasets import DatasetDict\n",
    "dataset = DatasetDict({\n",
    "\t\"train\": train_dataset,\n",
    "\t\"test\": test_dataset,\n",
    "\t\"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Number of Words')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"Description\"], df[\"Social\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "text_lengths = [len(t.split()) for t in train_texts]\n",
    "ax = sns.histplot(data=text_lengths, kde=True, stat=\"density\")\n",
    "ax.set_title(\"Distribution of Description lengths (number of words)\")\n",
    "ax.set_xlabel(\"Number of Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENCODING AND TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subjective', 'Gender', 'Jargon', 'Social']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# credit: https://github.com/NielsRogge/Transformers-Tutorials\n",
    "labels = [label for label in dataset[\"train\"].features.keys() if label not in [\"ObjectID\", \"Description\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# credit: https://github.com/NielsRogge/Transformers-Tutorials\n",
    "def preprocess_data(data):\n",
    "\n",
    "\t# save the given batch of descs\n",
    "\tdescs = data[\"Description\"]\n",
    "\n",
    "\t# encode them using bert tokenizer\n",
    "\tencoding = tokenizer(descs, padding=True, truncation=True, max_length=512)#.to(\"mps\")\n",
    "\n",
    "\t# create numpy array (no need to convert T/F to 0/1 since we annotated that way)\n",
    "\t# MATRIX FORMAT:\n",
    "\t# |---------------------------------\n",
    "\t# | bias   | bias1 bias2 bias3 bias4\n",
    "\t# |--------+------------------------\n",
    "\t# | desc0  |   1     0     1     0\n",
    "\t# | desc1  |   0     1     0     1\n",
    "\t# | desc2  |   0     1     0     0\n",
    "\t# | ...    |  ...   ...   ...   ...\n",
    "\t# \n",
    "\t# Convert integers to float and data to an NDarray\n",
    "\tsubjective = np.array(data[\"Subjective\"], dtype=float)\n",
    "\tgender = np.array(data[\"Gender\"], dtype=float)\n",
    "\tjargon = np.array(data[\"Jargon\"], dtype=float)\n",
    "\tsocial = np.array(data[\"Social\"], dtype=float)\n",
    "\t# Stack the arrays column-wise to form a 2D array (matrix)\n",
    "\tlabels_matrix = np.stack((subjective, gender, jargon, social), axis=1)\n",
    "\n",
    "\t\n",
    "\t# # Credit ChatGPT\n",
    "\t# # Validate the data stacking by comparing 3 random indices\n",
    "\t# import random\n",
    "\t# for _ in range(3):\n",
    "\t# \tidx = random.randint(0, len(subjective) - 1)\n",
    "\t# \tdataset_labels = [data[\"Subjective\"][idx], data[\"Gender\"][idx], data[\"Jargon\"][idx], data[\"Social\"][idx]]\n",
    "\t# \tmatrix_labels = labels_matrix[idx].tolist()\n",
    "\t# \tassert dataset_labels == matrix_labels, f\"Mismatch at index {idx}: {dataset_labels} != {matrix_labels}\"\n",
    "\t# \tprint(f\"Index {idx} matches: {dataset_labels}\")\n",
    "\n",
    "\n",
    "\t# FORMAT OF var encoding of type BatchEncoding (the length of the vals of each key \n",
    "\t# equal the num of descs/objects in given batch):\n",
    "\t# input_ids: [101, 1030, 4748, 7229, 1035, ...], ... (tokens/key/id for each BERT word)\n",
    "\t# token_type_ids: [0, 0, 0, 0, 0, ...], ... (defines the type of each token; we only have all zeros)\n",
    "\t# attention_mask: [1, 1, 1, 1, 1, ...], ... (tells model what to focus on by marking 1 for all tokens other than padding)\n",
    "\t# labels: [1.0, 1.0, 0.0, 0.0], ... (labels corresponding to the tokens)\n",
    "\tencoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "\treturn encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1442/1442 [00:05<00:00, 240.77 examples/s]\n",
      "Map: 100%|██████████| 180/180 [00:00<00:00, 462.42 examples/s]\n",
      "Map: 100%|██████████| 181/181 [00:00<00:00, 473.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] january 1991 alternate term added. object fumigated in orkin's piedmont vault with vikane in 1994 orkin's peidmont vault. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[0.0, 0.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# see example\n",
    "print(tokenizer.decode(encoded_dataset[\"train\"][6][\"input_ids\"]))\n",
    "print(encoded_dataset[\"train\"][6][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset a standard torch dataset by converting to tensors\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING MODEL + TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add Warmup?\n",
    "Experiment with learning rate\n",
    "experiment with batch size\n",
    "experient with gradient_accumulation_steps\n",
    "another metric?\n",
    "Choose another optimizer: RMSprop, SGD...\n",
    "Increase the learning rate by default and then use the callback ReduceLROnPlateau\n",
    "\"\"\"\n",
    "# use keras instead of huggung face to make it easier to work with messing with layers\n",
    "# remove entries greater than 512 words to remove noise\n",
    "# enchance data by repeatung key terms\n",
    "# cut 512 from middle of the dataset\n",
    "# try giving it only the labels with 5 word context\n",
    "# try doing subtext technique to give it 1000 words\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "lr = 2e-5\n",
    "metric_name = \"f1\"\n",
    "decay = 0.01\n",
    "\n",
    "model_path = \"./model\"\n",
    "tokenizer_path = \"./tokenizer\"\n",
    "logs_path = \"./logs\"\n",
    "\n",
    "# with open('hg_token.txt', 'r') as file:\n",
    "# \thg_token = file.read()\n",
    "hg_token = \"\"\n",
    "\n",
    "\n",
    "# args for training the model\n",
    "# save the model every epoch and choose the best performing epoch as the final version of the model\n",
    "args = TrainingArguments(\n",
    "\teval_strategy = \"epoch\",\n",
    "\tsave_strategy = \"epoch\",\n",
    "    # save_total_limit = 5,\n",
    "\tlogging_strategy = \"epoch\",\n",
    "\tlearning_rate = lr,\n",
    "\tper_device_train_batch_size = batch_size,\n",
    "\tper_device_eval_batch_size = batch_size,\n",
    "\tnum_train_epochs = num_epochs,\n",
    "\tweight_decay = decay,\n",
    "\tload_best_model_at_end = True,\n",
    "\tmetric_for_best_model = metric_name,\n",
    "\tlogging_dir = logs_path,\n",
    "\toutput_dir = model_path,\n",
    "    warmup_steps=100,\n",
    "\t# use_mps_device = True,\n",
    "\t# use_cpu = False,\n",
    "\tlogging_steps = 1,\n",
    "\t# gradient_accumulation_steps=2,\n",
    "\t\n",
    "\t# hub_token = hg_token,\n",
    "\t# hub_model_id = \"raasikhk/carlos_bert_v2_2\",\n",
    "\t# push_to_hub=True,\n",
    "\treport_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "from numpy import ndarray\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "def get_next_image_number(directory: str) -> int:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        return 1\n",
    "    images = glob.glob(os.path.join(directory, '*.png'))\n",
    "    if not images:\n",
    "        return 1\n",
    "    numbers = [int(os.path.basename(image).split('_')[0]) for image in images]\n",
    "    return max(numbers) + 1\n",
    "\n",
    "def plot_confusion_matrix(cm, save_path, title):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(cm))\n",
    "    plt.xticks(tick_marks, tick_marks, rotation=45)\n",
    "    plt.yticks(tick_marks, tick_marks)\n",
    "    \n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm[i])):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def my_accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[int, int, int, int]:\n",
    "    directory = 'cm'\n",
    "    image_number = get_next_image_number(directory)\n",
    "    \n",
    "    labels = [\"Subjective\", \"Gender\", \"Jargon\", \"Social\"]\n",
    "    true_pos_list, false_pos_list, true_neg_list, false_neg_list = [], [], [], []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        save_path = os.path.join(directory, f'{image_number}_{label}.png')\n",
    "        \n",
    "        # Calculate confusion matrix for the current label\n",
    "        cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(cm, save_path, f'Confusion Matrix - {label}')\n",
    "        \n",
    "        # Calculate true positives, false positives, true negatives, false negatives\n",
    "        true_pos = np.sum((y_true[:, i] == 1) & (y_pred[:, i] == 1))\n",
    "        false_pos = np.sum((y_true[:, i] == 0) & (y_pred[:, i] == 1))\n",
    "        true_neg = np.sum((y_true[:, i] == 0) & (y_pred[:, i] == 0))\n",
    "        false_neg = np.sum((y_true[:, i] == 1) & (y_pred[:, i] == 0))\n",
    "        \n",
    "        true_pos_list.append(true_pos)\n",
    "        false_pos_list.append(false_pos)\n",
    "        true_neg_list.append(true_neg)\n",
    "        false_neg_list.append(false_neg)\n",
    "    \n",
    "    return (\n",
    "        sum(true_pos_list), sum(false_pos_list), \n",
    "        sum(true_neg_list), sum(false_neg_list)\n",
    "    )\n",
    "\n",
    "\n",
    "def partial_accuracy_score(y_true: ndarray, y_pred: ndarray):\n",
    "\tnum_objects = len(y_true)\n",
    "\tnum_labels = len(y_true)*4\n",
    "\tcorrect_predictions = 0\n",
    "\t\n",
    "\tfor i in range(num_objects):\n",
    "\t\tfor j in range(len(y_true[i])):\n",
    "\t\t\tif y_true[i][j] == y_pred[i][j]:\n",
    "\t\t\t\tcorrect_predictions += 1\n",
    "\t\n",
    "\taccuracy = correct_predictions / num_labels\n",
    "\treturn accuracy\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "\t# first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "\tsigmoid = torch.nn.Sigmoid()\n",
    "\tprobs = sigmoid(torch.Tensor(predictions))\n",
    "\t# next, use threshold to turn them into integer predictions\n",
    "\ty_pred = np.zeros(probs.shape)\n",
    "\ty_pred[np.where(probs >= threshold)] = 1\n",
    "\t# finally, compute metrics\n",
    "\ty_true = labels\n",
    "\tf1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "\troc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "\taccuracy = accuracy_score(y_true, y_pred)\n",
    "\tmyacc = partial_accuracy_score(y_true, y_pred)\n",
    "\ttrue_pos, false_pos, true_neg, false_neg = my_accuracy_score(y_true, y_pred)\n",
    "\t# return as dictionary\n",
    "\tmetrics = {'f1': f1_micro_average,\n",
    "\t\t\t\t'roc_auc': roc_auc,\n",
    "\t\t\t\t'exact_match_acc': accuracy,\n",
    "\t\t\t\t\"partial_acc\": myacc,\n",
    "\t\t\t\t'true_pos': true_pos,\n",
    "        \t\t'true_neg': true_neg,\n",
    "        \t\t'false_neg': false_neg,\n",
    "\t\t\t\t'false_pos': false_pos}\n",
    "\treturn metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "\tpreds = p.predictions[0] if isinstance(p.predictions, \n",
    "\t\t\ttuple) else p.predictions\n",
    "\tresult = multi_label_metrics(\n",
    "\t\tpredictions=preds, \n",
    "\t\tlabels=p.label_ids)\n",
    "\treturn result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "\tmodel,\n",
    "\targs,\n",
    "\ttrain_dataset=encoded_dataset[\"train\"],\n",
    "\teval_dataset=encoded_dataset[\"validation\"],\n",
    "\ttokenizer=tokenizer,\n",
    "\tcompute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/460 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/transformers/trainer.py:3485\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3485\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/transformers/trainer.py:3532\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3531\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3532\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3533\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3534\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1696\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1694\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1696\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    584\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    436\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    438\u001b[0m )\n\u001b[0;32m--> 440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 7.77 GB, other allocations: 770.67 MB, max allowed: 9.07 GB). Tried to allocate 768.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/site-packages/transformers/trainer.py:3484\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3481\u001b[0m     loss_mb \u001b[38;5;241m=\u001b[39m smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n\u001b[1;32m   3482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 3484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m   3485\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/carlos/lib/python3.12/contextlib.py:778\u001b[0m, in \u001b[0;36mnullcontext.__exit__\u001b[0;34m(self, *excinfo)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menter_result\n\u001b[0;32m--> 778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mexcinfo):\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainOutput(global_step=5430, training_loss=0.039493819055855826, metrics={'train_runtime': 1080.8201, 'train_samples_per_second': 40.081, 'train_steps_per_second': 5.024, 'total_flos': 1.139817559375872e+16, 'train_loss': 0.039493819055855826, 'epoch': 30.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'eval_loss': 0.2363041341304779,\n",
    " 'eval_f1': 0.8076923076923078,\n",
    " 'eval_roc_auc': 0.8846526655896608,\n",
    " 'eval_exact_match_acc': 0.8232044198895028,\n",
    " 'eval_partial_acc': 0.9447513812154696,\n",
    " 'eval_true_pos': 84,\n",
    " 'eval_true_neg': 600,\n",
    " 'eval_false_neg': 21,\n",
    " 'eval_false_pos': 19,\n",
    " 'eval_runtime': 2.0471,\n",
    " 'eval_samples_per_second': 88.418,\n",
    " 'eval_steps_per_second': 11.235,\n",
    " 'epoch': 30.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view logs (only needed for analysis)\n",
    "# !pip install tensorboard\n",
    "!tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"model\")\n",
    "# tokenizer.save_pretrained(\"tokenizer\")\n",
    "# tokenizer = transformers.BertTokenizer.from_pretrained(\"tokenizer\")\n",
    "# model = transformers.BertForSequenceClassification.from_pretrained(\"model/checkpoint-1200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING NEWLY TRAINED MODEL BY HAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a description\n",
    "text = \"December 1992 lead-in term added. January 1991 alternate term added. Object fumigated in Orkin's Piedmont vault with Vikane in 1994\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "print(f\"encoding: {encoding}\")\n",
    "\n",
    "outputs = trainer.model(**encoding)\n",
    "# print(f\"outputs: {outputs}\")\n",
    "\n",
    "logits = outputs.logits\n",
    "print(f\"logits: {logits}\")\n",
    "\n",
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "print(f\"probs: {probs}\")\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING MODEL FROM HUGGINGFACE AND TESTING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only run this block below if you have not just trained the model here and would like to load our finetuned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST THE MODEL\n",
    "# IF YOU DONT WANT TO TRAIN THEN LOAD MODEL FROM HUGGINGFACE\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# if tokenizer == None:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"raasikhk/carlos_bert_v2_2\", revision=\"4f6590dd149a1cf31d0cc09fa6e2db13fdfc15f1\")\n",
    "# if model == None:\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"raasikhk/carlos_bert_v2_2\", revision=\"4f6590dd149a1cf31d0cc09fa6e2db13fdfc15f1\", output_attentions=True)\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=None, truncation=True, padding=True, device=device)\n",
    "\n",
    "\"\"\"Sinker\n",
    "\n",
    "NOTE: UPDATED in June 2021 with information from 1984 accession worksheet.\n",
    "\n",
    "A February 14, 1997 file memo from registrar Lori Iliff discussed how sources and provenance were determined for objects that were believed to have an Etowah or Etowah-related provenance.  This memo did not discuss the X.0600-X.0697 numbered objects, as the sources and provenance of those objects had been discussed in a March 1990 memo.  Information regarding the objects in the 1997 memo came from the Accession Log I (green-blue colored ledger) and old accession sheets. According to Lori's memo it was unclear when the accession log was started, but it was on or prior to 1984.  The accession worksheets had been created in 1984.\n",
    "\n",
    "According to Lori's memo, the designation of Etowah as provenance and Phillips Academy as source on the accession worksheets seemed arbitrary and not based on prior museum records.  Therefore the designation of Etowah or Phillips Academy based solely on an accession worksheet may be suspect. Below is a summary of what was found for this object.\n",
    "\n",
    "X.0232.024 - According to the memo, the accession log lists no source, but lists Etowah as provenance.  The memo does not mention an accession worksheet. There is a 1984 accession worksheet in the object file.  It notes the provenance as Georgia, Etowah Mounds and does not note a source.\n",
    "\n",
    "The memos from the 1990s do not mention the \"Specimen Record\" worksheets housed in the blue fabric-covered binders. A few of these worksheets contain notes added by Lori Iliff in 1994.  It is unclear why these worksheets are not mentioned in the 1990s memos, but for completeness, their information is noted here.  It is however, unclear when these worksheets were created, by whom, and where the information in the worksheets came from.  Many have notes regarding packing objects for storage in 1982, so it can be assumed these were created in 1982 or earlier. There is one Specimen Record for X.0232.023, X.0232.024, X.0280.001, X.0280.002, X.0281 and X.0282.  The Specimen Record does not list a provenance, and W.H. Ferguson is listed as the source.\n",
    "\n",
    "The catalog for the 1982 \"A Preview of the Collections\"exhibition in Schatten Gallery lists the credit line for X.0232.024 as \"Gift of W.H. Ferguson\", but there is no indication as to where this information came from.\n",
    "\n",
    ".\n",
    "\n",
    "Luminescence induced by the absorption of infrared radiation, visible light, or ultraviolet radiation.  RHDEL2.\n",
    "\n",
    "Identified as Jasper by William Size.\"\"\"\n",
    "\n",
    "\n",
    "texts = [\"\"\"A Chewa boy in Malawi must undergo a three-day initiation in order to achieve full status as an adult. Masks, such as this one, may be commissioned from a recognized carver by a friend or relative, or by the initiate himself... The mischievous characters interact with and perform for the audience to teach moral lessons and enforce social norms. This extraordinary example is carved from a dense, oily hardwood and sparingly decorated with red European paint. Its commanding presence is marked by a strong brow, varying textures and materials in the beard, and a rather wild full head of hair.\"\"\"]\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipe(texts)\n",
    "\n",
    "# Print the predictions\n",
    "for i, text in enumerate(texts):\n",
    "    # print(f\"Text: {text}\")\n",
    "    print(f\"Predictions: {predictions[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALCULATING ACCURACY AND OTHER METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make progressbar\n",
    "import sys\n",
    "def print_progress_bar(percentage):\n",
    "    bar_length = 40 \n",
    "    block = int(bar_length * (percentage / 100))\n",
    "    progress_bar = \"█\" * block + \" \" * (bar_length - block)\n",
    "    sys.stdout.write(f\"\\r[{progress_bar}] {percentage:.2f}%\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE ACCURACY\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "unclean = False # test on filtered descriptions or original, noisy ones\n",
    "\n",
    "test_on = \"validation\" # choose what to test on\n",
    "test_on = \"test\"\n",
    "\n",
    "print(\"OPTIONS:\")\n",
    "print(f\"unclean data = {unclean}\")\n",
    "print(f\"testing set = {test_on}\\n\")\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"raasikhk/carlos_bert_v2_2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"raasikhk/carlos_bert_v2_2\")\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=None, truncation=True, padding=True, device=device)\n",
    "\n",
    "print(\"Getting data...\")\n",
    "if unclean:\n",
    "\tunclean_df = pd.read_excel(\"../../../carlos_data/preprocessed_data_v3.xlsx\")\n",
    "\tunclean_df = unclean_df.drop(['Social', 'Jargon', 'Subjective', 'Gender'], axis=1)\n",
    "\n",
    "\t# Replace with test dataset\n",
    "\ttest_split = dataset[test_on]\n",
    "\ttest_split = pd.DataFrame(test_split)\n",
    "\ttest_split = test_split.drop(['Social', 'Jargon', 'Subjective', 'Gender'], axis=1)\n",
    "\n",
    "\t# Merge test_descriptions with unclean_df on ObjectID\n",
    "\tmerged_df = test_split.merge(unclean_df, on=\"ObjectID\", suffixes=('', '_unclean'), how='left')\n",
    "\t# Extract the newly replaced descriptions\n",
    "\tnew_descriptions = merged_df[\"TextEntry\"].values\n",
    "\n",
    "\t# Get list of unclean descriptions\n",
    "\ttest_descriptions = new_descriptions.tolist()\n",
    "else:\n",
    "    test_descriptions = dataset[test_on][\"Description\"]\n",
    "\n",
    "# make predictions variable of the format\n",
    "# -----------------------\n",
    "# bias1 bias2 bias3 bias4\n",
    "# -----------------------\n",
    "#  1     0     1     0\n",
    "#  0     1     0     1\n",
    "#  0     1     0     0\n",
    "# ...   ...   ...   ...\n",
    "predictions = np.zeros((len(test_descriptions), 4), int)\n",
    "\n",
    "print(\"Getting predictions...\")\n",
    "c = 0 # counter for progress bar\n",
    "for i in range(len(test_descriptions)):\n",
    "    pred = pipe(test_descriptions[i])\n",
    "    for j in range(4):\n",
    "        label = pred[0][j][\"label\"]\n",
    "        score = pred[0][j][\"score\"]\n",
    "        if label == \"Subjective\":\n",
    "            predictions[i][0] = 1 if score >= 0.5 else 0\n",
    "        elif label == \"Gender\":\n",
    "            predictions[i][1] = 1 if score >= 0.5 else 0\n",
    "        elif label == \"Jargon\":\n",
    "            predictions[i][2] = 1 if score >= 0.5 else 0\n",
    "        elif label == \"Social\":\n",
    "            predictions[i][3] = 1 if score >= 0.5 else 0\n",
    "    c += 1\n",
    "    print_progress_bar(c/len(test_descriptions)*100)\n",
    "\n",
    "# merge classifications of each bias column wise to create matrix:\n",
    "# subj0   gend0   jarg0   soci0\n",
    "# subj0   gend0   jarg0   soci0\n",
    "# subj0   gend0   jarg0   soci0\n",
    "true_values = np.column_stack((dataset[test_on][\"Subjective\"], dataset[test_on][\"Gender\"], \n",
    "                               dataset[test_on][\"Jargon\"], dataset[test_on][\"Social\"]))\n",
    "\n",
    "print(\"\\n\\nAccuracy Calculations:\")\n",
    "# Use Scikit-learn method\n",
    "print(f\"Accuracy: {accuracy_score(true_values, predictions)}\")\n",
    "\n",
    "# Calculate partial accuracy\n",
    "part_acc_score = 0\n",
    "total = true_values.size  # Or predictions.size, since both have the same shape\n",
    "\n",
    "for i in range(true_values.shape[0]):\n",
    "    for j in range(true_values.shape[1]):\n",
    "        if true_values[i][j] == predictions[i][j]:\n",
    "            part_acc_score += 1\n",
    "\n",
    "print(f\"Partial Accuracy: {part_acc_score/total}\")\n",
    "print(f\"F1 Score: {f1_score(true_values, predictions, average='micro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy per label\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy_per_label(predictions, true_values):\n",
    "    accuracies = {}\n",
    "    labels = [\"Subjective\", \"Gender\", \"Jargon\", \"Social\"]\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        correct_predictions = np.sum(predictions[:, i] == true_values[:, i])\n",
    "        total_predictions = predictions.shape[0]\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        accuracies[label] = accuracy\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "# Example usage\n",
    "accuracies = calculate_accuracy_per_label(predictions, true_values)\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions for validation or test set (depending on choice above) as csv\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(predictions, columns=[\"Subjective\", \"Gender\", \"Jargon\", \"Social\"])\n",
    "\n",
    "# Add the descriptions as the first column (choose unclean in block above)\n",
    "if unclean:\n",
    "\tdf.insert(0, \"Description\", test_descriptions)\n",
    "else:\n",
    "\tdf.insert(0, \"FilteredDescription\", test_descriptions)\n",
    "\n",
    "# add objectID\n",
    "df.insert(0, \"ObjectID\", dataset[test_on][\"ObjectID\"])\n",
    "\n",
    "df.to_csv(f\"carlos_data/bert_{test_on}_predictions.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW IS RANDOM EXPERIMENTAL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store confusion matrices for each label\n",
    "confusion_matrices = []\n",
    "\n",
    "# Iterate over each label (column in the matrices)\n",
    "for i in range(true_values.shape[1]):\n",
    "    # Compute confusion matrix for the current label\n",
    "    cm = confusion_matrix(true_values[:, i], predictions[:, i])\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "# Print confusion matrices for each label\n",
    "for i, cm in enumerate(confusion_matrices):\n",
    "    print(f\"Confusion Matrix for Label {i}:\")\n",
    "    print(cm)\n",
    "    print()\n",
    "\n",
    "    # Optional: Plot the confusion matrix\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix for Label {i}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "\n",
    "# Example text\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize and prepare input\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Forward pass with attention output\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "attentions = outputs.attentions  # List of attention scores for each layer\n",
    "\n",
    "# Function to visualize attention for the last layer\n",
    "def visualize_attention(attentions, input_ids, tokenizer, layer_idx=-1):\n",
    "    attention = attentions[layer_idx].squeeze().detach().numpy()  # Get attention scores for the specified layer\n",
    "    # Token mapping\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    \n",
    "    # Plot attention for the first attention head in the layer\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(attention[0], xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
    "    plt.title(f'Attention Map - Layer {layer_idx + 1}')\n",
    "    plt.xlabel('Token')\n",
    "    plt.ylabel('Token')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize attention for the last layer\n",
    "visualize_attention(attentions, input_ids, tokenizer, layer_idx=-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carlos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
