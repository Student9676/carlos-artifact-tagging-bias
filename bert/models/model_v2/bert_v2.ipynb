{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install pandas transformers torch datasets numpy openpyxl scikit-learn\n",
    "#  !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 1280\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 160\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['ObjectID', 'Description', 'Subjective', 'Gender', 'Jargon', 'Social'],\n",
      "        num_rows: 161\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers, torch\n",
    "\n",
    "\"\"\"\n",
    "Remove uneecessay new lines\n",
    "remove duplicates within one cell\n",
    "Remove links?\n",
    "use keywords to make sure that correct classification is done\n",
    "make validation set more class balanced, i.e. equal num of examples for each category\n",
    "\"\"\"\n",
    "# load data and rename TextEntry column\n",
    "df = pd.read_excel(\"../../../carlos_data/preprocessed_data_v2.xlsx\")\n",
    "df = df.rename(columns={\"TextEntry\":\"Description\"})\n",
    "\n",
    "# convert data to dictionary\n",
    "data = df.to_dict(\"records\")\n",
    "\n",
    "# Split the data into train and validation and test sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "train_dict, test_dict = train_test_split(data, test_size=0.20, random_state=42)\n",
    "test_dict, validation_dict = train_test_split(test_dict, test_size=0.50, random_state=42)\n",
    "\n",
    "# Create Dataset objects\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_list(train_dict)\n",
    "test_dataset = Dataset.from_list(test_dict)\n",
    "validation_dataset = Dataset.from_list(validation_dict)\n",
    "\n",
    "# Create DatasetDict\n",
    "from datasets import DatasetDict\n",
    "dataset = DatasetDict({\n",
    "\t\"train\": train_dataset,\n",
    "\t\"test\": test_dataset,\n",
    "\t\"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Number of Words')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAHHCAYAAABEEKc/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdZklEQVR4nO3deVhUZf8G8PvMwDDsi8gmCIS4GygoorgViUslZblUbplWaunrVpqKZYlLmksqWW+avZm+/irrtSQJt1LCFctUVERBEXBj35nn9wdxcg6ggMiA3p/rmgvmOc8553sehuHmbCMJIQSIiIiISKYydAFEREREDQ0DEhEREZECAxIRERGRAgMSERERkQIDEhEREZECAxIRERGRAgMSERERkQIDEhEREZECAxIRERGRAgMSUSMkSRImTZpUb+vbu3cvJEnC3r177/u65s+fD0mS9Nrqc3s3btwISZJw8eLFelmfUk5ODhwcHPDVV18ZZP01VT5eR44cMXQp1XLu3Dn07dsX1tbWkCQJ27dvN3RJNSZJEubPn1+jeYYNG4YhQ4bcn4IeUAxIVCuSJFXrUVd/UFNSUjB//nzExcXVyfIAYPTo0bCwsKiz5dW1gwcPYv78+cjIyKjT5V68eFHvZ2RsbAx7e3t069YNs2fPRlJSUp2ta+HChQ32D1BDrW3lypWwtLTEsGHD5Lby0Ojo6Ii8vLwK83h4eODJJ5+szzIbrVGjRuHPP//EBx98gC+//BL+/v4V+qSnp0OSJEyePLnCtMmTJ0OSJISFhVWYNnLkSBgbG1f6MzK0t956C9988w1OnDhh6FIaDQYkqpUvv/xS7/HEE09U2t6mTZs6WV9KSgrefffdOg1IDd3Bgwfx7rvv1nlAKjd8+HB8+eWX+Pe//425c+fikUcewYoVK9CmTRts2bJFr2/Pnj2Rn5+Pnj171mgdtQkhc+bMQX5+fo3mqY2qahsxYgTy8/Ph7u5+32tQKi4uxsqVK/HKK69ArVZXmJ6eno5169bVe10Pivz8fMTExGDs2LGYNGkSXnrpJbi6ulbo5+DgAG9vb/z2228Vph04cABGRkY4cOBApdM6duwIMzOz+1L/vejYsSP8/f2xbNkyQ5fSaBgZugBqnF566SW957///juioqIqtFPD1alTpwo/r0uXLqFv374YNWoU2rRpAx8fHwCASqWCVqu9r/Xk5ubC3NwcRkZGMDIy3FuTWq2uNJzUhx07duDatWtVHgrx9fXF0qVLMWHCBJiamtZzdYZV/vq4F9euXQMA2NjY3LVvUFAQNm3ahJycHHlPc25uLk6cOIEhQ4bghx9+QGlpqfxauXr1Ki5cuIBBgwbdU43l67nXba3MkCFDEBYWhrVr1zbovecNBfcg0X2j0+mwYsUKtGvXDlqtFo6Ojnj11Vdx69YtuU9YWBhUKhWio6P15h0/fjw0Gg1OnDiBvXv3onPnzgCAMWPGyIeGNm7cCKDsnILBgwfDyckJWq0Wrq6uGDZsGDIzM+tkO2JjY9GvXz9YW1vDzMwMvXr1qvDfY/khkPPnz2P06NGwsbGBtbU1xowZU2F3e35+Pt58803Y29vD0tISTz/9NK5cuaJ3XsH8+fMxY8YMAICnp6e8zcrzYrZv34727dvDxMQE7dq1Q2Rk5D1tq7u7OzZu3IiioiIsWbJEbq/sHKS7jbskScjNzcUXX3wh1z969Gi98Tp16hReeOEF2NraIigoSG9aZb766iu0atUKWq0Wfn5+2L9/v9700aNHw8PDo8J8ymXeqbaqzkFau3Yt2rVrBxMTE7i4uGDixIkV9u717t0b7du3x6lTp9CnTx+YmZmhWbNmemN5J9u3b4eHhwe8vLwqnT5v3jykpaXddS9SVeeMlR9eLf/dAf451JyUlIQnn3wSFhYWaNasGdasWQMA+PPPP/HYY4/B3Nwc7u7u2Lx5c6XrzMvLw6uvvoomTZrAysoKI0eO1PtdL7dz50706NED5ubmsLS0xMCBA/HXX3/p9SmvKSEhAQMGDIClpSVefPHFO27z8ePH0b9/f1hZWcHCwgKPP/44fv/9d3n6/Pnz5b2CM2bMgCRJlb5WygUFBaG0tFRvGbGxsSgpKcH06dORk5Ojt0e7/D2h/HUMANu2bYOfnx9MTU1hb2+Pl156CVeuXKn2thYWFuJf//oXmjZtKr9XXL58uUKt2dnZmDJlCjw8PGBiYgIHBwc88cQTOHbsmF6/J554Arm5uYiKirrjWFIZBiS6b1599VXMmDED3bt3x8qVKzFmzBh89dVXCAkJQXFxMYCywym+vr4YO3YssrOzAQA///wzPv30U8ybNw8+Pj5o06YN3nvvPQBlwan88F3Pnj1RVFSEkJAQ/P7773jjjTewZs0ajB8/HhcuXKiTQ1O7d+9Gz549kZWVhbCwMCxcuBAZGRl47LHHcOjQoQr9hwwZguzsbISHh2PIkCHYuHEj3n33Xb0+o0ePxurVqzFgwAAsXrwYpqamGDhwoF6fZ599FsOHDwcAfPTRR/I2N23aVO7z22+/YcKECRg2bBiWLFmCgoICDB48GDdu3LinbQ4MDISXl9cd30SrM+5ffvklTExM0KNHD7n+V199VW85zz//PPLy8rBw4UKMGzfujnXt27cPU6ZMwUsvvYT33nsPN27cQL9+/XDy5Mkab2N1arvd/PnzMXHiRLi4uGDZsmUYPHgwPvnkE/Tt21d+LZe7desW+vXrBx8fHyxbtgytW7fGW2+9hZ07d961roMHD6JTp05VTu/Rowcee+wxLFmypE4PQ5aWlqJ///5wc3PDkiVL4OHhgUmTJmHjxo3o168f/P39sXjxYlhaWmLkyJFITEyssIxJkybh9OnTmD9/PkaOHImvvvoKoaGhEELIfb788ksMHDgQFhYWWLx4MebOnYtTp04hKCioQiAtKSlBSEgIHBwc8OGHH2Lw4MFV1v/XX3+hR48eOHHiBGbOnIm5c+ciMTERvXv3RmxsLICy36mPPvoIwD+Hl1esWFHlMsuDzu2H2Q4cOICWLVuiY8eOcHV11ftHSRmQNm7ciCFDhkCtViM8PBzjxo3Dt99+i6CgoArvTVVt6yuvvIIVK1agb9++WLRoEYyNjSu8VwDAa6+9hnXr1mHw4MFYu3Ytpk+fDlNTU5w+fVqvX9u2bWFqalrp4UGqhCCqAxMnThS3v5x+/fVXAUB89dVXev0iIyMrtP/5559Co9GIV155Rdy6dUs0a9ZM+Pv7i+LiYrnP4cOHBQCxYcMGveUdP35cABDbtm2rcc2jRo0S5ubmVU7X6XTC29tbhISECJ1OJ7fn5eUJT09P8cQTT8htYWFhAoB4+eWX9ZbxzDPPiCZNmsjPjx49KgCIKVOm6PUbPXq0ACDCwsLktqVLlwoAIjExsUJtAIRGoxHnz5+X206cOCEAiNWrV99xuxMTEwUAsXTp0ir7DBo0SAAQmZmZQggh9uzZIwCIPXv2CCGqP+7m5uZi1KhRFdrLx2v48OFVTrsdAAFAHDlyRG67dOmS0Gq14plnnpHbRo0aJdzd3au1zKpq27Bhg964p6enC41GI/r27StKS0vlfh9//LEAID7//HO5rVevXgKA2LRpk9xWWFgonJycxODBgyus63bFxcVCkiQxbdq0Kuu/du2a2LdvnwAgli9fLk93d3cXAwcOlJ8rf17lyn/2t/8ejRo1SgAQCxculNtu3bolTE1NhSRJYsuWLXL7mTNnKrxOy8fLz89PFBUVye1LliwRAMT3338vhBAiOztb2NjYiHHjxunVlJqaKqytrfXay2t6++237zhm5UJDQ4VGoxEJCQlyW0pKirC0tBQ9e/assP13eu3fzsHBQTz++OPy85CQEDFmzBghhBBDhgwRzz//vDzN399feHt7CyGEKCoqEg4ODqJ9+/YiPz9f7rNjxw4BQMybN++u2xoXFycAiAkTJui1v/DCCxV+BtbW1mLixInV2qaWLVuK/v37V6vvw457kOi+2LZtG6ytrfHEE0/g+vXr8sPPzw8WFhbYs2eP3Ld9+/Z499138dlnnyEkJATXr1/HF198Ua3zUKytrQGU7XWq6ytH4uLicO7cObzwwgu4ceOGvA25ubl4/PHHsX//fuh0Or15XnvtNb3nPXr0wI0bN5CVlQUA8iGwCRMm6PV74403alxfcHCw3qGYRx99FFZWVrhw4UKNl6VUfn5C+V49pboad+V43UlgYCD8/Pzk582bN8egQYPw888/o7S0tNY13M0vv/yCoqIiTJkyBSrVP2+Z48aNg5WVFX788Ue9/hYWFnrndmk0GnTp0uWuP5ebN29CCAFbW9s79uvZsyf69OlT53uRXnnlFfl7GxsbtGrVCubm5nrnQ7Vq1Qo2NjaVbsv48eNhbGwsP3/99ddhZGSEn376CQAQFRWFjIwMDB8+XO89Qa1WIyAgQO894fZl3E1paSl27dqF0NBQPPLII3K7s7MzXnjhBfz222/y719Nde/eHbGxsSgtLYVOp8Pvv/+Obt26ydPK98Tk5eUhLi5O3nt05MgRpKenY8KECXrn7g0cOBCtW7eu8JqpbFvLx+3NN9/Ua58yZUqFeW1sbBAbG4uUlJS7bpOtrS2uX79+137EQ2x0n5w7dw6ZmZlwcHBA06ZN9R45OTlIT0/X6z9jxgz4+Pjg0KFDCAsLQ9u2bau1Hk9PT0ydOhWfffYZ7O3tERISgjVr1tTJ+Ufnzp0DUHZZsHIbPvvsMxQWFlZYT/PmzfWel/+xKz8X49KlS1CpVPD09NTr16JFixrXp1xX+foqO++jpnJycgAAlpaWlU6vq3FXjsOdeHt7V2hr2bIl8vLy5JNv74dLly4BKAsHt9NoNHjkkUfk6eVcXV0rnENVk5+LuO2QVFXmz5+P1NRUREREVGuZd6PVavUO3wJlIbiybbG2tq50W5Q/HwsLCzg7O8uHzsp/nx577LEKv0+7du2q8J5gZGRU6RVmSteuXUNeXl6Fnw8AtGnTBjqdDsnJyXddTmWCgoLkc41OnjyJzMxMdO/eHQDQrVs3pKSk4OLFi/K5SeUBqarXDAC0bt26wmumsm0tf69Qno9W2TKXLFmCkydPws3NDV26dMH8+fOrDORCiCrP8SN9vIqN7gudTnfHm90p34wvXLggv4H++eefNVrXsmXLMHr0aHz//ffYtWsX3nzzTYSHh+P333+v1htsVcr3Di1duhS+vr6V9lFeCVLV1U/V+aNXU/dzXSdPnoSDgwOsrKyq7FMX417XV2JV9cZ/P/cwKdX252JnZwdJkqoVpHr27InevXtjyZIlle6Fq+k4VFVzXb7Gyn+fvvzySzg5OVWYrtxjbGJiorfHzhBuPw9Jo9HAzs4OrVu3BlB2RaGZmRl+++03+Zys20/Qrol73dYhQ4agR48e+O6777Br1y4sXboUixcvxrfffov+/fvr9b1161al/2xQRQxIdF94eXnhl19+Qffu3e/6R1Cn02H06NGwsrLClClTsHDhQjz33HN49tln5T53+4+nQ4cO6NChA+bMmYODBw+ie/fuiIiIwPvvv39P2wAAVlZWCA4OrvVybufu7g6dTofExES9N6nz589X6Guo//JiYmKQkJBQrVs23G3c63IbygP07c6ePQszMzM5cNva2lZ6cr7yP/aa1FZ+5VN8fLzeIZyioiIkJibW2WvDyMgIXl5elZ4AXZn58+ejd+/e+OSTTypMK99zqRyLysahrpw7dw59+vSRn+fk5ODq1asYMGAAgH9+nxwcHOpszICyf7bMzMwQHx9fYdqZM2egUqng5uZWq2V36tRJDkEmJiYIDAyUXzdGRkbo3LkzDhw4gMTERDg4OKBly5YA9F8zjz32mN4y4+Pjq3WPrfL3ioSEBL29RpVtJ1B2SHHChAmYMGEC0tPT0alTJ3zwwQd6AamkpATJycl4+umnazYQDykeYqP7YsiQISgtLcWCBQsqTCspKdF7416+fDkOHjyI9evXY8GCBejWrRtef/11vePk5fcEUb7hZ2VloaSkRK+tQ4cOUKlUKCwsvKdt8PPzg5eXFz788EP5kNPtanNYJyQkBEDZJeO3W716dYW+VW3z/XTp0iWMHj0aGo1Gvs1AZao77ubm5nVWf0xMjN5ly8nJyfj+++/Rt29feU+Hl5cXMjMz8ccff8j9rl69iu+++67C8qpbW3BwMDQaDVatWqW35+Tf//43MjMzK72qqLYCAwOr/ZEdvXr1Qu/evbF48WIUFBToTXN3d4dara5wGwTl664urV+/Xu+KvnXr1qGkpET+Ax0SEgIrKyssXLiwwpV/QO1+n4CyvVx9+/bF999/r3clXFpaGjZv3oygoKA77gm9EyMjIwQEBODAgQM4cOCAfP5RuW7dumH//v34/fff5UNvAODv7w8HBwdERETo/T7s3LkTp0+frtZrpnzcVq1apdeuvPKutLS0wqFtBwcHuLi4VHgPPHXqFAoKCipsB1WOe5DovujVqxdeffVVhIeHIy4uDn379oWxsTHOnTuHbdu2YeXKlXjuuedw+vRpzJ07F6NHj8ZTTz0FoOzyWF9fX0yYMAH//e9/AZT94bOxsUFERAQsLS1hbm6OgIAAnDhxApMmTcLzzz+Pli1boqSkBF9++SXUavUdLwsuV1xcXOleJjs7O0yYMAGfffYZ+vfvj3bt2mHMmDFo1qwZrly5gj179sDKygr/+9//ajQufn5+GDx4MFasWIEbN26ga9eu2LdvH86ePQtAf69G+QnJ77zzDoYNGwZjY2M89dRTdXYDuWPHjuE///kPdDodMjIycPjwYXzzzTeQJAlffvklHn300Srn3b17d7XG3c/PD7/88guWL18OFxcXeHp6IiAgoFb1tm/fHiEhIXjzzTdhYmIi/7G//TYKw4YNw1tvvYVnnnkGb775JvLy8rBu3Tq0bNmywj1hqltb06ZNMWvWLLz77rvo168fnn76acTHx2Pt2rXo3Llznd4cddCgQfjyyy9x9uxZeW/EnYSFhenttSlnbW2N559/HqtXr4YkSfDy8sKOHTsqnOdTl4qKivD4449jyJAh8vgEBQXJeyusrKywbt06jBgxAp06dcKwYcPQtGlTJCUl4ccff0T37t3x8ccf12rd77//PqKiohAUFIQJEybAyMgIn3zyCQoLC6t9D6qqBAUFySeQ3x6CgLKAFB4eLvcrZ2xsjMWLF2PMmDHo1asXhg8fjrS0NKxcuRIeHh7417/+ddf1+vr6Yvjw4Vi7di0yMzPRrVs3REdHV9jbnJ2dDVdXVzz33HPw8fGBhYUFfvnlFxw+fLjCXbOjoqJgZmYmf/IB3YXhLqCjB4nyMv9y69evF35+fsLU1FRYWlqKDh06iJkzZ4qUlBRRUlIiOnfuLFxdXUVGRobefCtXrhQAxNatW+W277//XrRt21YYGRnJlypfuHBBvPzyy8LLy0totVphZ2cn+vTpI3755Ze71lx+eW1lDy8vL7nf8ePHxbPPPiuaNGkiTExMhLu7uxgyZIiIjo6W+9x+GfbtlJeMCyFEbm6umDhxorCzsxMWFhYiNDRUxMfHCwBi0aJFevMvWLBANGvWTKhUKr3lAKj0sl53d/dKL12/XfmlzuUPIyMjYWdnJwICAsSsWbPEpUuXKsyjvGy8uuN+5swZ0bNnT2FqaioAyLVVNV63T7td+fb+5z//Ed7e3sLExER07NixwmXsQgixa9cu0b59e6HRaESrVq3Ef/7zn0qXWVVtlf3MhCi7rL9169bC2NhYODo6itdff13cunVLr0+vXr1Eu3btKtRU1e0HlAoLC4W9vb1YsGBBpWNS2XiV31rg9sv8hRDi2rVrYvDgwcLMzEzY2tqKV199VZw8ebLSy/wru91FVduivKVA+Xjt27dPjB8/Xtja2goLCwvx4osvihs3blSYf8+ePSIkJERYW1sLrVYrvLy8xOjRo/Vu4XC3W3BU5tixYyIkJERYWFgIMzMz0adPH3Hw4EG9PjW9zF8IIX7++Wf59yQ3N1dv2o0bN4QkSQKAiI2NrTDv1q1bRceOHYWJiYmws7MTL774orh8+bJenztta35+vnjzzTdFkyZNhLm5uXjqqadEcnKy3mX+hYWFYsaMGcLHx0dYWloKc3Nz4ePjI9auXVtheQEBAeKll16q9rY/7CQh7sPZo0RUI3FxcejYsSP+85//3PWOwfRgW7BgATZs2IBz584Z7CNP6METFxeHTp064dixY1VedEL6eA4SUT2r7N41K1asgEqlqvGHwdKD51//+hdycnIqfGAw0b1YtGgRnnvuOYajGuAeJKJ69u677+Lo0aPo06cPjIyMsHPnTuzcuRPjx4+v9IokIiKqfwxIRPUsKioK7777Lk6dOoWcnBw0b94cI0aMwDvvvGPQT7EnIqJ/MCARERERKfAcJCIiIiIFBiQiIiIiBZ7wUEs6nQ4pKSmwtLTkB/8RERE1EkIIZGdnw8XF5Y6fgceAVEspKSm1/nwfIiIiMqzk5OQ7frA2A1ItWVpaAigb4Np+zg8RERHVr6ysLLi5ucl/x6vCgFRL5YfVrKysGJCIiIgambudHsOTtImIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUDB6Q1qxZAw8PD2i1WgQEBODQoUN37L9t2za0bt0aWq0WHTp0wE8//aQ3/dtvv0Xfvn3RpEkTSJKEuLi4CssoKCjAxIkT0aRJE1hYWGDw4MFIS0ury80iIiKiRsygAWnr1q2YOnUqwsLCcOzYMfj4+CAkJATp6emV9j948CCGDx+OsWPH4vjx4wgNDUVoaChOnjwp98nNzUVQUBAWL15c5Xr/9a9/4X//+x+2bduGffv2ISUlBc8++2ydbx8RERE1TpIQQhhq5QEBAejcuTM+/vhjAIBOp4ObmxveeOMNvP322xX6Dx06FLm5udixY4fc1rVrV/j6+iIiIkKv78WLF+Hp6Ynjx4/D19dXbs/MzETTpk2xefNmPPfccwCAM2fOoE2bNoiJiUHXrl2rVXtWVhasra2RmZnJD6slIiJqJKr799tge5CKiopw9OhRBAcH/1OMSoXg4GDExMRUOk9MTIxefwAICQmpsn9ljh49iuLiYr3ltG7dGs2bN7/jcgoLC5GVlaX3ICIiogeTwQLS9evXUVpaCkdHR712R0dHpKamVjpPampqjfpXtQyNRgMbG5saLSc8PBzW1tbyw83NrdrrrInS0lKcPXtWfpSWlt6X9RAREVHVDH6SdmMxa9YsZGZmyo/k5OT7sp6EhAQs++ZXbDiQiGXf/IqEhIT7sh4iIiKqmpGhVmxvbw+1Wl3h6rG0tDQ4OTlVOo+Tk1ON+le1jKKiImRkZOjtRbrbckxMTGBiYlLt9dwLOydXOLh61su6iIiIqCKD7UHSaDTw8/NDdHS03KbT6RAdHY3AwMBK5wkMDNTrDwBRUVFV9q+Mn58fjI2N9ZYTHx+PpKSkGi2HiIiIHlwG24MEAFOnTsWoUaPg7++PLl26YMWKFcjNzcWYMWMAACNHjkSzZs0QHh4OAJg8eTJ69eqFZcuWYeDAgdiyZQuOHDmC9evXy8u8efMmkpKSkJKSAqAs/ABle46cnJxgbW2NsWPHYurUqbCzs4OVlRXeeOMNBAYGVvsKNiIiInqwGTQgDR06FNeuXcO8efOQmpoKX19fREZGyidiJyUlQaX6ZydXt27dsHnzZsyZMwezZ8+Gt7c3tm/fjvbt28t9fvjhBzlgAcCwYcMAAGFhYZg/fz4A4KOPPoJKpcLgwYNRWFiIkJAQrF27th62mIiIiBoDg94HqTG7X/dBOnv2LDYcSISDqyfSLydiTHdPtGzZss6WT0RE9DBr8PdBIiIiImqoGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQMHpDWrFkDDw8PaLVaBAQE4NChQ3fsv23bNrRu3RparRYdOnTATz/9pDddCIF58+bB2dkZpqamCA4Oxrlz5/T6nD17FoMGDYK9vT2srKwQFBSEPXv21Pm2ERERUeNk0IC0detWTJ06FWFhYTh27Bh8fHwQEhKC9PT0SvsfPHgQw4cPx9ixY3H8+HGEhoYiNDQUJ0+elPssWbIEq1atQkREBGJjY2Fubo6QkBAUFBTIfZ588kmUlJRg9+7dOHr0KHx8fPDkk08iNTX1vm8zERERNXySEEIYauUBAQHo3LkzPv74YwCATqeDm5sb3njjDbz99tsV+g8dOhS5ubnYsWOH3Na1a1f4+voiIiICQgi4uLhg2rRpmD59OgAgMzMTjo6O2LhxI4YNG4br16+jadOm2L9/P3r06AEAyM7OhpWVFaKiohAcHFyt2rOysmBtbY3MzExYWVnd61DIzp49iw0HEuHg6on0y4kY090TLVu2rLPlExERPcyq+/fbYHuQioqKcPToUb1AolKpEBwcjJiYmErniYmJqRBgQkJC5P6JiYlITU3V62NtbY2AgAC5T5MmTdCqVSts2rQJubm5KCkpwSeffAIHBwf4+flVWW9hYSGysrL0HkRERPRgMlhAun79OkpLS+Ho6KjX7ujoWOWhrtTU1Dv2L/96pz6SJOGXX37B8ePHYWlpCa1Wi+XLlyMyMhK2trZV1hseHg5ra2v54ebmVrMNJiIiokbD4Cdp1zchBCZOnAgHBwf8+uuvOHToEEJDQ/HUU0/h6tWrVc43a9YsZGZmyo/k5OR6rJqIiIjqk8ECkr29PdRqNdLS0vTa09LS4OTkVOk8Tk5Od+xf/vVOfXbv3o0dO3Zgy5Yt6N69Ozp16oS1a9fC1NQUX3zxRZX1mpiYwMrKSu9BREREDyaDBSSNRgM/Pz9ER0fLbTqdDtHR0QgMDKx0nsDAQL3+ABAVFSX39/T0hJOTk16frKwsxMbGyn3y8vIAlJ3vdDuVSgWdTnfvG0ZERESNnpEhVz516lSMGjUK/v7+6NKlC1asWIHc3FyMGTMGADBy5Eg0a9YM4eHhAIDJkyejV69eWLZsGQYOHIgtW7bgyJEjWL9+PYCy84umTJmC999/H97e3vD09MTcuXPh4uKC0NBQAGUhy9bWFqNGjcK8efNgamqKTz/9FImJiRg4cKBBxoGIiIgaFoMGpKFDh+LatWuYN28eUlNT4evri8jISPkk66SkJL09Pd26dcPmzZsxZ84czJ49G97e3ti+fTvat28v95k5cyZyc3Mxfvx4ZGRkICgoCJGRkdBqtQDKDu1FRkbinXfewWOPPYbi4mK0a9cO33//PXx8fOp3AIiIiKhBMuh9kBoz3geJiIio8Wnw90EiIiIiaqgYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUDB6Q1qxZAw8PD2i1WgQEBODQoUN37L9t2za0bt0aWq0WHTp0wE8//aQ3XQiBefPmwdnZGaampggODsa5c+cqLOfHH39EQEAATE1NYWtri9DQ0LrcLCIiImrEDBqQtm7diqlTpyIsLAzHjh2Dj48PQkJCkJ6eXmn/gwcPYvjw4Rg7diyOHz+O0NBQhIaG4uTJk3KfJUuWYNWqVYiIiEBsbCzMzc0REhKCgoICuc8333yDESNGYMyYMThx4gQOHDiAF1544b5vLxERETUOkhBCGGrlAQEB6Ny5Mz7++GMAgE6ng5ubG9544w28/fbbFfoPHToUubm52LFjh9zWtWtX+Pr6IiIiAkIIuLi4YNq0aZg+fToAIDMzE46Ojti4cSOGDRuGkpISeHh44N1338XYsWNrXXtWVhasra2RmZkJKyurWi9H6ezZs9hwIBEOrp5Iv5yIMd090bJlyzpbPhER0cOsun+/DbYHqaioCEePHkVwcPA/xahUCA4ORkxMTKXzxMTE6PUHgJCQELl/YmIiUlNT9fpYW1sjICBA7nPs2DFcuXIFKpUKHTt2hLOzM/r376+3F6oyhYWFyMrK0nsQERHRg8lgAen69esoLS2Fo6OjXrujoyNSU1MrnSc1NfWO/cu/3qnPhQsXAADz58/HnDlzsGPHDtja2qJ37964efNmlfWGh4fD2tpafri5udVga4mIiKgxMfhJ2vVNp9MBAN555x0MHjwYfn5+2LBhAyRJwrZt26qcb9asWcjMzJQfycnJ9VUyERER1TODBSR7e3uo1WqkpaXptaelpcHJyanSeZycnO7Yv/zrnfo4OzsDANq2bStPNzExwSOPPIKkpKQq6zUxMYGVlZXeg4iIiB5MBgtIGo0Gfn5+iI6Oltt0Oh2io6MRGBhY6TyBgYF6/QEgKipK7u/p6QknJye9PllZWYiNjZX7+Pn5wcTEBPHx8XKf4uJiXLx4Ee7u7nW2fURERNR4GdVmpgsXLuCRRx6555VPnToVo0aNgr+/P7p06YIVK1YgNzcXY8aMAQCMHDkSzZo1Q3h4OABg8uTJ6NWrF5YtW4aBAwdiy5YtOHLkCNavXw8AkCQJU6ZMwfvvvw9vb294enpi7ty5cHFxke9zZGVlhddeew1hYWFwc3ODu7s7li5dCgB4/vnn73mbiIiIqPGrVUBq0aIFevXqhbFjx+K5556DVqut1cqHDh2Ka9euYd68eUhNTYWvry8iIyPlk6yTkpKgUv2zk6tbt27YvHkz5syZg9mzZ8Pb2xvbt29H+/bt5T4zZ85Ebm4uxo8fj4yMDAQFBSEyMlKvxqVLl8LIyAgjRoxAfn4+AgICsHv3btja2tZqO4iIiOjBUqv7IMXFxWHDhg34+uuvUVRUhKFDh2Ls2LHo0qXL/aixQeJ9kIiIiBqf+3ofJF9fX6xcuRIpKSn4/PPPcfXqVQQFBaF9+/ZYvnw5rl27VuvCiYiIiAztnk7SNjIywrPPPott27Zh8eLFOH/+PKZPnw43NzeMHDkSV69eras6iYiIiOrNPQWkI0eOYMKECXB2dsby5csxffp0JCQkICoqCikpKRg0aFBd1UlERERUb2p1kvby5cuxYcMGxMfHY8CAAdi0aRMGDBggn1Dt6emJjRs3wsPDoy5rJSIiIqoXtQpI69atw8svv4zRo0fLN15UcnBwwL///e97Ko6IiIjIEGoVkKKiotC8eXO9S/ABQAiB5ORkNG/eHBqNBqNGjaqTIomIiIjqU63OQfLy8sL169crtN+8eROenp73XBQRERGRIdUqIFV166ScnJxa3zSSiIiIqKGo0SG2qVOnAij7SI958+bBzMxMnlZaWorY2Fj4+vrWaYFERERE9a1GAen48eMAyvYg/fnnn9BoNPI0jUYDHx8fTJ8+vW4rJCIiIqpnNQpIe/bsAQCMGTMGK1eurNOP2CAiIiJqKGp1FduGDRvqug4iIiKiBqPaAenZZ5/Fxo0bYWVlhWefffaOfb/99tt7LoyIiIjIUKodkKytrSFJkvw9ERER0YOq2gHp9sNqPMRGRERED7Ja3QcpPz8feXl58vNLly5hxYoV2LVrV50VRkRERGQotQpIgwYNwqZNmwAAGRkZ6NKlC5YtW4ZBgwZh3bp1dVogERERUX2rVUA6duwYevToAQD4v//7Pzg5OeHSpUvYtGkTVq1aVacFEhEREdW3WgWkvLw8WFpaAgB27dqFZ599FiqVCl27dsWlS5fqtEAiIiKi+largNSiRQts374dycnJ+Pnnn9G3b18AQHp6Om8eSURERI1erQLSvHnzMH36dHh4eCAgIACBgYEAyvYmdezYsU4LJCIiIqpvtbqT9nPPPYegoCBcvXoVPj4+cvvjjz+OZ555ps6KIyIiIjKEWgUkAHBycoKTk5NeW5cuXe65ICIiIiJDq1VAys3NxaJFixAdHY309HTodDq96RcuXKiT4oiIiIgMoVYB6ZVXXsG+ffswYsQIODs7yx9BQkRERPQgqFVA2rlzJ3788Ud07969rushIiIiMrhaXcVma2sLOzu7uq6FiIiIqEGoVUBasGAB5s2bp/d5bEREREQPilodYlu2bBkSEhLg6OgIDw8PGBsb600/duxYnRRHREREZAi1CkihoaF1XAYRERFRw1GrgBQWFlbXdRARERE1GLU6BwkAMjIy8Nlnn2HWrFm4efMmgLJDa1euXKmz4oiIiIgMoVZ7kP744w8EBwfD2toaFy9exLhx42BnZ4dvv/0WSUlJ2LRpU13XSURERFRvarUHaerUqRg9ejTOnTsHrVYrtw8YMAD79++vs+KIiIiIDKFWAenw4cN49dVXK7Q3a9YMqamp91wUERERkSHVKiCZmJggKyurQvvZs2fRtGnTey6KiIiIyJBqFZCefvppvPfeeyguLgYASJKEpKQkvPXWWxg8eHCdFkhERERU32oVkJYtW4acnBw0bdoU+fn56NWrF1q0aAFLS0t88MEHdV0jERERUb2q1VVs1tbWiIqKwoEDB3DixAnk5OSgU6dOCA4Oruv6iIiIiOpdjQOSTqfDxo0b8e233+LixYuQJAmenp5wcnKCEAKSJN2POomIiIjqTY0OsQkh8PTTT+OVV17BlStX0KFDB7Rr1w6XLl3C6NGj8cwzz9yvOomIiIjqTY32IG3cuBH79+9HdHQ0+vTpozdt9+7dCA0NxaZNmzBy5Mg6LZKIiIioPtVoD9LXX3+N2bNnVwhHAPDYY4/h7bffxldffVVnxREREREZQo0C0h9//IF+/fpVOb1///44ceLEPRdFREREZEg1Ckg3b96Eo6NjldMdHR1x69atey6KiIiIyJBqFJBKS0thZFT1aUtqtRolJSX3XBQRERGRIdXoJG0hBEaPHg0TE5NKpxcWFtZJUURERESGVKOANGrUqLv24RVsRERE1NjVKCBt2LDhftVBRERE1GDU6rPYiIiIiB5kDEhERERECgxIRERERAoMSEREREQKDEhERERECgxIRERERAoMSEREREQKDEhERERECgxIRERERAo1upM21S+drhSJiYnycy8vL6jVagNWRERE9HBoEHuQ1qxZAw8PD2i1WgQEBODQoUN37L9t2za0bt0aWq0WHTp0wE8//aQ3XQiBefPmwdnZGaampggODsa5c+cqXVZhYSF8fX0hSRLi4uLqapPqREb6VWzcewobDiRi2Te/IiEhwdAlERERPRQMHpC2bt2KqVOnIiwsDMeOHYOPjw9CQkKQnp5eaf+DBw9i+PDhGDt2LI4fP47Q0FCEhobi5MmTcp8lS5Zg1apViIiIQGxsLMzNzRESEoKCgoIKy5s5cyZcXFzu2/bdKxsHFzi4esLOydXQpRARET00DB6Qli9fjnHjxmHMmDFo27YtIiIiYGZmhs8//7zS/itXrkS/fv0wY8YMtGnTBgsWLECnTp3w8ccfAyjbe7RixQrMmTMHgwYNwqOPPopNmzYhJSUF27dv11vWzp07sWvXLnz44Yf3ezOJiIioETFoQCoqKsLRo0cRHBwst6lUKgQHByMmJqbSeWJiYvT6A0BISIjcPzExEampqXp9rK2tERAQoLfMtLQ0jBs3Dl9++SXMzMzqcrOIiIiokTNoQLp+/TpKS0vh6Oio1+7o6IjU1NRK50lNTb1j//Kvd+ojhMDo0aPx2muvwd/fv1q1FhYWIisrS+9BREREDyaDH2IzhNWrVyM7OxuzZs2q9jzh4eGwtraWH25ubvexQiIiIjIkgwYke3t7qNVqpKWl6bWnpaXBycmp0nmcnJzu2L/865367N69GzExMTAxMYGRkRFatGgBAPD398eoUaMqXe+sWbOQmZkpP5KTk2u4tURERNRYGDQgaTQa+Pn5ITo6Wm7T6XSIjo5GYGBgpfMEBgbq9QeAqKgoub+npyecnJz0+mRlZSE2Nlbus2rVKpw4cQJxcXGIi4uTbxOwdetWfPDBB5Wu18TEBFZWVnoPIiIiejAZ/EaRU6dOxahRo+Dv748uXbpgxYoVyM3NxZgxYwAAI0eORLNmzRAeHg4AmDx5Mnr16oVly5Zh4MCB2LJlC44cOYL169cDACRJwpQpU/D+++/D29sbnp6emDt3LlxcXBAaGgoAaN68uV4NFhYWAMpuxOjqysvpiYiIHnYGD0hDhw7FtWvXMG/ePKSmpsLX1xeRkZHySdZJSUlQqf7Z0dWtWzds3rwZc+bMwezZs+Ht7Y3t27ejffv2cp+ZM2ciNzcX48ePR0ZGBoKCghAZGQmtVlvv20dERESNjySEEIYuojHKysqCtbU1MjMz6/Rw29mzZ7HhQCIcXD1x5sivUGkt0LJ9R6RfTsSY7p5o2bJlna2LiIjoYVPdv98P5VVsRERERHfCgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkwIBEREREpMCARERERKTAgERERESkYGToAqjmSktLkZCQID/38vKCWq02YEVEREQPFgakRighIQHLvvkVdk6uuJl6GdMGAy1btjR0WURERA8MBqRGys7JFQ6unoYug4iI6IHEc5CIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBpEQFqzZg08PDyg1WoREBCAQ4cO3bH/tm3b0Lp1a2i1WnTo0AE//fST3nQhBObNmwdnZ2eYmpoiODgY586dk6dfvHgRY8eOhaenJ0xNTeHl5YWwsDAUFRXdl+0jIiKixsXgAWnr1q2YOnUqwsLCcOzYMfj4+CAkJATp6emV9j948CCGDx+OsWPH4vjx4wgNDUVoaChOnjwp91myZAlWrVqFiIgIxMbGwtzcHCEhISgoKAAAnDlzBjqdDp988gn++usvfPTRR4iIiMDs2bPrZZtrQ6crRWJiIs6ePYvExERA6AxdEhER0QNLEkIIQxYQEBCAzp074+OPPwYA6HQ6uLm54Y033sDbb79dof/QoUORm5uLHTt2yG1du3aFr68vIiIiIISAi4sLpk2bhunTpwMAMjMz4ejoiI0bN2LYsGGV1rF06VKsW7cOFy5cqFbdWVlZsLa2RmZmJqysrGq62VU6e/YsNhxIhIOrJ84c+RUqrQVatu+IM0d+RVZWFjxatsWFk0dg59oCLdt3RPrlRIzp7smPGiEiIqqG6v79NugepKKiIhw9ehTBwcFym0qlQnBwMGJiYiqdJyYmRq8/AISEhMj9ExMTkZqaqtfH2toaAQEBVS4TKAtRdnZ2VU4vLCxEVlaW3qO+2Ti4wMHVEzb2TvW+biIiooeJQQPS9evXUVpaCkdHR712R0dHpKamVjpPamrqHfuXf63JMs+fP4/Vq1fj1VdfrbLW8PBwWFtbyw83N7c7bxwRERE1WgY/B8nQrly5gn79+uH555/HuHHjquw3a9YsZGZmyo/k5OR6rJKIiIjqk0EDkr29PdRqNdLS0vTa09LS4ORU+WEkJyenO/Yv/1qdZaakpKBPnz7o1q0b1q9ff8daTUxMYGVlpfcgIiKiB5NBA5JGo4Gfnx+io6PlNp1Oh+joaAQGBlY6T2BgoF5/AIiKipL7e3p6wsnJSa9PVlYWYmNj9ZZ55coV9O7dG35+ftiwYQNUqod+ZxoRERH9zcjQBUydOhWjRo2Cv78/unTpghUrViA3NxdjxowBAIwcORLNmjVDeHg4AGDy5Mno1asXli1bhoEDB2LLli04cuSIvAdIkiRMmTIF77//Pry9veHp6Ym5c+fCxcUFoaGhAP4JR+7u7vjwww9x7do1uZ6q9lwRERHRw8PgAWno0KG4du0a5s2bh9TUVPj6+iIyMlI+yTopKUlv7063bt2wefNmzJkzB7Nnz4a3tze2b9+O9u3by31mzpyJ3NxcjB8/HhkZGQgKCkJkZCS0Wi2Asj1O58+fx/nz5+Hq6qpXj4HvekBEREQNgMHvg9RYGeI+SJV9z/sgERERVV+juA8SERERUUPEgERERESkwIBEREREpMCARERERKTAgERERESkYPDL/OnOCkolxCTcQHKeNUSBEa7Ep8O81Ag2hi6MiIjoAcaA1EAVlpTidL4lLmeaQXf9JgBTAEDq5UwA9rAtLIV9bpFBayQiInpQ8RBbA1RYKvDtsStIKjKHDhKcrbVoqc1CG8tCtHSwgASBW8VqbD2SjNRcnaHLJSIieuAwIDUwN/JKEJ1UivTsQhhLOvjb5ON5P1d4muTBw6wY/Ts4o4flNdgal6KoRIe9l3X45XyWocsmIiJ6oDAgNSBFJTrM2HkZmUWAuYkaXcxvoKlJKSRJ0utnqtKhs20+WjtZQgBY/lsaDiXeNEzRREREDyAGpAZEY6TCsEftYGEMPO/nBgt1aZV91RLQt60jmltKKNEBr/3nKJJv5tVjtURERA8uBqQGpq+3Ffp7qGFtanzXvpIkIcBJhRZNTHAztwjjNh1BYUnVoYqIiIiqhwGpATJSSXfvdFvfd4Od0cRcgzOp2VgVfe4+VkZERPRwYEBq5HS6UuSkX8aEADsAwLq9CTiRnGHYooiIiBo5BqRGLiP9KjbuPYXzV67DyTgfOgFM23YCBcU81EZERFRbDEgPABsHFzi4eqKbuwVsTdU4n56Dz369YOiyiIiIGi0GpAeIiVrCq13sAQAf7zmPKxn5Bq6IiIiocWJAesD0ecQSXTzsUFCsw8IfTxu6HCIiokaJAekBI0kS5j/dDioJ+PHPqzh4/rqhSyIiImp0GJAeQG1drPBSV3cAwIIfT6NUJwxcERERUePCgPSA+ldwS1hpjXD6aha+OXrZ0OUQERE1KgxIDyhbcw3efNwbALB0VzxyC0sMXBEREVHjwYD0ABsR6I7mdma4ll2IT/bzsn8iIqLqYkB6gJkYqTGrf2sAwPr9Cbiaycv+iYiIqoMB6QHXr70TOnvYoqBYh6U/xxu6HCIiokaBAekBJ0kS5gxsCwD49tgV/Hk508AVERERNXwMSA8BHzcbhPq6AADe//EUhOBl/0RERHfCgPSQmNGvNUyMVIhNvImoU2mGLoeIiKhBY0B6SDSzMcUrPTwBAOE7z6CoRGfgioiIiBouBqSHyOu9W8DeQoPE67n4z++XDF0OERFRg8WA9ADR6UqRmJiIs2fP4uzZsygtLdWbbmFihGl9WwEAVkafQ0ZekSHKJCIiavAYkB4gGelXsXHvKWw4kIhl3/yKhISECn2G+LuhlaMlMvOLsXr3eQNUSURE1PAxID1gbBxc4ODqCTsn10qnq1US3hnYBgCwKeYiLl7Prc/yiIiIGgUGpIdQz5ZN0btVUxSXCizaecbQ5RARETU4DEgPqdkD2kAlAZF/peL3CzcMXQ4REVGDwoD0kGrpaInhXZoDAOZuP4nCktK7zEFERPTwYEB6iM0IaQV7Cw3OpecgYu8FQ5dDRETUYDAgPcRszDSY91Q7AMCaPedxPj3bwBURERE1DAxID7mnHnVGn1ZNUVSqw8z/+wMlpbzDNhEREQPSQ06SJLz/TAdYmhjhWFIG1uypeO8kIiKihw0DEqGZjSkWhLYHAKzafQ7Hkm4ZuCIiIiLDYkAiAEBox2YY5OuCUp3AlC1xyMwvNnRJREREBsOARLIFoe3hamuKpJt5mLzlOEp1wtAlERERGQQDEsmstMaIeMkPWmMV9sZfw7Jd8YYuiYiIyCAYkAgAUFpairNnz0KTm4Z/dXMAAKzdm4BtR5INXBkREVH9Y0AiAEBCQgKWffMrNhxIxLG4OAx91BYA8NY3fyDyZKqBqyMiIqpfDEgPKJ2uFImJiTh79izOnj2L0tK7f5SInZMrHFw9Yefkipf9mmCIvyt0Anjz6+PYd/ZaPVRNRETUMDAgPaAy0q9i495T2HAgEcu++RUJCTW7v5EkSVj4TAf0a+eEolIdXvniMHb8kXKfqiUiImpYjAxdAN0/Ng4ucHD1rPF85XufAOANfwuoVE746c9UvPH1caRnFWJMdw9IklTX5RIRETUYDEhUQUb6VWw8nwWPFOBm6mVMeSYIdubN8Z/fk/DejlP443IGFj7bAWYavnyIiOjBxENsVKnyvU92Tq5QqyQsGNQeswe0hlolYXtcCgZ9fAAnkjMMXSYREdF9wYD0EKjNCdtKkiRhfE8vbH4lAE0tTXAuPQfPrD2A93ecQnYB77pNREQPFh4jeQgoD5lNGwy0bNmyWvPefj4SAPh7eeHnKT3x3v/+wva4FHz2WyK+O34Fkx5rgRcCmsPESH2/NoOIiKjeMCA9JMoPmd0eeMr3JKnV6rI2oaswX1XhasWwjnja1wXv7ziNC9dz8e7/TmHd3gQMbGmBga2sYaVVw8vLC2o1AxMRETU+DEgPmdsDz4WTR6DSmMGjZVtcOHkEdq4t4OBWcZ7KwhUA9PL2Qo9/9cR/jyRjVfQ5pGUVYsPRQnxx7AaaqgswoXcOhvT0gammbkJSaWmp3u0KGMCIiOh+YUB6CJUHnpupl6HSWsjf301Ve5NeDHDH835u+GzXMXwam4ZbhUBaiRZhv1zFwr1pCPRqgj6tHPBYawe42ZnVuu7yu33bObnW+FAhERFRTTAgUY1UdW8ljZEKwS2skJx2A5K1C46eS8atIjXSc0uwN/4a9sZfQ9gPf8HFWotHXW3g42YDHzdrtG9mDSutcbXXX363byIiovuJAYlqpbJzmZKSkgChQ1NLE/jaA32aCahsm+NQci7+vKHDkUsZSMksQEpmKiL/+ufz3ZystPByMEeLphZo4WABNzszuNqawsXG9L7fa4mH7YiIqDIMSFQrlZ3LpCvKk89jyki/ii/OZ8GjpYSbqZexYHAPuLh3wR/Jt7D7xAWcvVaAM9cKkJ5bgtSsAqRmFeDA+RsV1mNjZoxmNn+HJRQg8YYO6apMFOfoEH+tAOYO+bC30NT66jketiMioso0iIC0Zs0aLF26FKmpqfDx8cHq1avRpUuXKvtv27YNc+fOxcWLF+Ht7Y3FixdjwIAB8nQhBMLCwvDpp58iIyMD3bt3x7p16+Dt7S33uXnzJt544w3873//g0qlwuDBg7Fy5UpYWFjc1219kCjPZdIV5FQ6/fa9TdmJiUiKT0YzZzcUZhyBu7EFbJq3QsKFRBQYWUCyaILMvCIIIxPkFumQkVeMjLxi/JWS9c+Cr6cDAPZfSQb+lwwAMDdWwdZUDRtTNVyaWMPWTAMbc+Oyr6bGsDHTwMbs7+dmxrAxM5ZDFQ/bERGRksED0tatWzF16lREREQgICAAK1asQEhICOLj4+Hg4FCh/8GDBzF8+HCEh4fjySefxObNmxEaGopjx46hffv2AIAlS5Zg1apV+OKLL+Dp6Ym5c+ciJCQEp06dglarBQC8+OKLuHr1KqKiolBcXIwxY8Zg/Pjx2Lx5c71u/8NAubfJzrWFXrBq6f0IVJlXoNJq0LJ9C6Qmncdjrio4NPNAem4J1JZNcTWrECcvXMGhixkQxqa4mZGJglKgBMbQAcgt1iG3WIfLWcU4mVZQrbrMNGqYGUkoKimB6dUkoKQUydEpcIzLh7mJCsV52TA3VsHUWAVPV2dYaI2hNVbDxEgFE2M1tMYqmBj9/dxIBa2xGkaSwKWL/1zpV9khu4ZwWK+ua2gI20S1x58fUUUGD0jLly/HuHHjMGbMGABAREQEfvzxR3z++ed4++23K/RfuXIl+vXrhxkzZgAAFixYgKioKHz88ceIiIiAEAIrVqzAnDlzMGjQIADApk2b4OjoiO3bt2PYsGE4ffo0IiMjcfjwYfj7+wMAVq9ejQEDBuDDDz+Ei4tLPW39w+P2vU13IweqlsD1lCQ8598cAZ6ecMjMhqpAgoNbM5w5cgEqrQW827XBn4cPoERjAQf3ljh3+i9k5xfCwt4ZmZlZcGliCWFsivTMPKTllKBEMkJRqQAgIa+oFHlFZevMKioEAKRdygUu5VZS1bVqb6sEAbUkAUIHU+NzMDZSQ60C1JIEI5UEXWkxsvOLYawxRmlREZpanoO5mSnUEqCWACO1CmqVBBsrCxir1VCrJBirJagkCSoA2dlZUEll61FJgJFKBUmSYN/EFkZ/91dJEtQqAELg1s0bUEkSIARUqrLl37h+Hccu3oSlrR1yM27i6a7ZcHF2QnpaGiSprNZmzZxhrDaC6u/aVfJypb+fQ/7+0qVL2LjrCGybOiHz+lW8OqAY3l5ekCrpX74MlSSVbcc9fPBxVX/YK2sHUOsQUNMA0dgCBw81G15je808DAwakIqKinD06FHMmjVLblOpVAgODkZMTEyl88TExGDq1Kl6bSEhIdi+fTsAIDExEampqQgODpanW1tbIyAgADExMRg2bBhiYmJgY2MjhyMACA4OhkqlQmxsLJ555pk63EqqjdsD1ca9p/T3Pt12ryZJkqBRCWiNBFxtzZCjKUAzKwu0bO+FM0d+RVZaCjxbtgUuxKG1awu0bN8RackX8HxndzRxbo6TZxPw7fErMLd1RHraVXhZq2BqZYuUazeRkCmgNrVEdlY2LDUSCkoFrucUQzIyRlFxKUy1GpQICQXFpSguFXJNAhJKBACokF0MoLiyj3YxBgoAQIPMTACZ+ZX0yamk7U5u1rC/DXBVB8AGf+xJBZCqmJ5Sw+U1BXJKAThg/9aLAC5Way6VVP4oC0xGapUconQ6HSSgLLSp1ZAkCRKAkpISSFLZH5X8olKo1WroSkthaZYIjbExSkqKkZlbCLWRGrqSElibJ0KSJGTk5kOtNoKutAS2Fheh0WgACBQXF0NCWQ0mJpq/aykLdSpJQmFBAVJvZcPYWIOS4iI0b3oZ5mZm8nTp79rLtyE3NxcJKTdgojVFUWEeWrtehY211d99/+lX2byS9E94VKukuwZIIXTIuJUBALCxtYEkqaqcXt4HimXeunkLSWpnXC+wRJ7aGZ8dvg7bhFIInUBGxm3z2thAUlVdj4Sy7Sv/md3+HH//7G5vvz0gl7cLCAhRXnvZ9zoB6HQ63LhZ9hoXAKxtbCBJEoQoey4EUKrT4datWwAAnU5A/L1gIQAra6u/l4+/lynk7/H3OsvbdUIgKysLf0+CuYWFvBzxd106nUB2Tk7Zc52urO/fyzczMwMgQScEcnNzoft7e0xNTeXl6P7etvLtzcvPx+VrmVBrNCgpKoJLk2SYaLV/11VWq04nUFBYKI9N+agDKHstSxKEECgqLCyrC39v4N/t5f3LlyfptZc9N9Foyl53AIqLi/7+Zwy3vV5R9g+dSiU/L++fn5cn//NW/s+RlaXlP/8Qqcpe1wCQk50lv85tbKxhpFJDgkBmZgbUfy/Xzs4Wz/s3h6e9eZWvufvJoAHp+vXrKC0thaOjo167o6Mjzpw5U+k8qamplfZPTU2Vp5e33amP8vCdkZER7Ozs5D5KhYWFKPz7hQkAmZmZAICsrKxK+9dWTk4OUi+eQ2F+Lq5duQiVxgymJsaN4ntdUd59W3Zhfi6KCwtwLfnCPc97Ky0Fl2zyIBXnoyj1IkR6CoylfBQnHMfhrGw4u3kgJeEMbFzc0bxFK1xMO47srGyoiwrQ6u+2W2kpeKqDCzw8PHDx4kX8EJcCKwdnXDzzJ2BsBmd3LySdPw3JWAsnN09cToyHZKSFQzN3XL10HpKRCeyd3ZCanADJSIsmTs2QfvkiYGQCWwdnXL+ahIKCIlja2iE/Nxe+LdxgZW2NW7cyEJ+eDRMzS2RcTwfUxrC0sUXWjWsoKiqGqaUlcrMyAZURtBYWyM/OgrGpOUwtrJGbdaus3cwS+TkZECojaE0tUFxUADtzE+gA3MgtgpGxBsXFxbDSGsPI2BgFhcXIyC2ASq1GSXExIKkgGRlBV1ICIamgUqlRqtNBUqkgSWroRNmbowDkPwx3UvH+7bWXV1BYaXtuQdFtz8q+z8kvqrTvnZUAAG5eugGg4kUFFVdctjfy6pl0AOm1WF9NXbm3PlfL6v3ram4V/aqz/Ppy973R+moa+G+Xdg/z1lDe36+xvJr+w9NQVed1f7WK9ivo4GCCJhr7uixI/rt9ezisjMEPsTUW4eHhePfddyu0u7lVcutpavA2NpBlVFdsPa6LiKihGLDi/i07Ozsb1tbWVU43aECyt7eHWq1GWpp+Ok9LS4OTk1Ol8zg5Od2xf/nXtLQ0ODs76/Xx9fWV+6Sn66fakpIS3Lx5s8r1zpo1S+/Qnk6nw82bN9GkSZN7OodCKSsrC25ubkhOToaVlVWdLfdhxLGsGxzHusOxrBscx7rzMI6lEALZ2dl3Pd/YoAFJo9HAz88P0dHRCA0NBVAWPKKjozFp0qRK5wkMDER0dDSmTJkit0VFRSEwMBAA4OnpCScnJ0RHR8uBKCsrC7GxsXj99dflZWRkZODo0aPw8/MDAOzevRs6nQ4BAQGVrtfExAQmJiZ6bTY2NrXc8ruzsrJ6aF6s9xvHsm5wHOsOx7JucBzrzsM2lnfac1TO4IfYpk6dilGjRsHf3x9dunTBihUrkJubK1/VNnLkSDRr1gzh4eEAgMmTJ6NXr15YtmwZBg4ciC1btuDIkSNYv349gLKTzKZMmYL3338f3t7e8mX+Li4ucghr06YN+vXrh3HjxiEiIgLFxcWYNGkShg0bxivYiIiIyPABaejQobh27RrmzZuH1NRU+Pr6IjIyUj7JOikpCSrVP1dldOvWDZs3b8acOXMwe/ZseHt7Y/v27fI9kABg5syZyM3Nxfjx45GRkYGgoCBERkbK90ACgK+++gqTJk3C448/Lt8octWqVfW34URERNRgSeJup3FTvSosLER4eDhmzZpV4ZAe1QzHsm5wHOsOx7JucBzrDseyagxIRERERAqqu3chIiIiergwIBEREREpMCARERERKTAgERERESkwIDUwa9asgYeHB7RaLQICAnDo0CFDl2RQ+/fvx1NPPQUXFxdIkiR/KHE5IQTmzZsHZ2dnmJqaIjg4GOfOndPrc/PmTbz44ouwsrKCjY0Nxo4di5wc/Q+C/eOPP9CjRw9otVq4ublhyZIl93vT6lV4eDg6d+4MS0tLODg4IDQ0FPHx8Xp9CgoKMHHiRDRp0gQWFhYYPHhwhbvWJyUlYeDAgTAzM4ODgwNmzJiBkpISvT579+5Fp06dYGJighYtWmDjxo33e/Pqzbp16/Doo4/KN9ULDAzEzp075ekcw9pZtGiRfA+7chzL6pk/f37Zhzjf9mjdurU8neN4DwQ1GFu2bBEajUZ8/vnn4q+//hLjxo0TNjY2Ii0tzdClGcxPP/0k3nnnHfHtt98KAOK7777Tm75o0SJhbW0ttm/fLk6cOCGefvpp4enpKfLz8+U+/fr1Ez4+PuL3338Xv/76q2jRooUYPny4PD0zM1M4OjqKF198UZw8eVJ8/fXXwtTUVHzyySf1tZn3XUhIiNiwYYM4efKkiIuLEwMGDBDNmzcXOTk5cp/XXntNuLm5iejoaHHkyBHRtWtX0a1bN3l6SUmJaN++vQgODhbHjx8XP/30k7C3txezZs2S+1y4cEGYmZmJqVOnilOnTonVq1cLtVotIiMj63V775cffvhB/Pjjj+Ls2bMiPj5ezJ49WxgbG4uTJ08KITiGtXHo0CHh4eEhHn30UTF58mS5nWNZPWFhYaJdu3bi6tWr8uPatWvydI5j7TEgNSBdunQREydOlJ+XlpYKFxcXER4ebsCqGg5lQNLpdMLJyUksXbpUbsvIyBAmJibi66+/FkIIcerUKQFAHD58WO6zc+dOIUmSuHLlihBCiLVr1wpbW1tRWFgo93nrrbdEq1at7vMWGU56eroAIPbt2yeEKBs3Y2NjsW3bNrnP6dOnBQARExMjhCgLqyqVSqSmpsp91q1bJ6ysrOSxmzlzpmjXrp3euoYOHSpCQkLu9yYZjK2trfjss884hrWQnZ0tvL29RVRUlOjVq5cckDiW1RcWFiZ8fHwqncZxvDc8xNZAFBUV4ejRowgODpbbVCoVgoODERMTY8DKGq7ExESkpqbqjZm1tTUCAgLkMYuJiYGNjQ38/f3lPsHBwVCpVIiNjZX79OzZExqNRu4TEhKC+Ph43Lp1q562pn5lZmYCAOzs7AAAR48eRXFxsd5Ytm7dGs2bN9cbyw4dOsh3uQfKxikrKwt//fWX3Of2ZZT3eRBfw6WlpdiyZQtyc3MRGBjIMayFiRMnYuDAgRW2l2NZM+fOnYOLiwseeeQRvPjii0hKSgLAcbxXDEgNxPXr11FaWqr3IgUAR0dHpKamGqiqhq18XO40ZqmpqXBwcNCbbmRkBDs7O70+lS3j9nU8SHQ6HaZMmYLu3bvLH9GTmpoKjUZT4QOYlWN5t3Gqqk9WVhby8/Pvx+bUuz///BMWFhYwMTHBa6+9hu+++w5t27blGNbQli1bcOzYMflzNm/Hsay+gIAAbNy4EZGRkVi3bh0SExPRo0cPZGdncxzvkcE/i42I6tfEiRNx8uRJ/Pbbb4YupVFq1aoV4uLikJmZif/7v//DqFGjsG/fPkOX1agkJydj8uTJiIqK0vuMTKq5/v37y98/+uijCAgIgLu7O/773//C1NTUgJU1ftyD1EDY29tDrVZXuLogLS0NTk5OBqqqYSsflzuNmZOTE9LT0/Wml5SU4ObNm3p9KlvG7et4UEyaNAk7duzAnj174OrqKrc7OTmhqKgIGRkZev2VY3m3caqqj5WV1QPzZq3RaNCiRQv4+fkhPDwcPj4+WLlyJcewBo4ePYr09HR06tQJRkZGMDIywr59+7Bq1SoYGRnB0dGRY1lLNjY2aNmyJc6fP8/X5D1iQGogNBoN/Pz8EB0dLbfpdDpER0cjMDDQgJU1XJ6ennByctIbs6ysLMTGxspjFhgYiIyMDBw9elTus3v3buh0OgQEBMh99u/fj+LiYrlPVFQUWrVqBVtb23ramvtLCIFJkybhu+++w+7du+Hp6ak33c/PD8bGxnpjGR8fj6SkJL2x/PPPP/UCZ1RUFKysrNC2bVu5z+3LKO/zIL+GdTodCgsLOYY18Pjjj+PPP/9EXFyc/PD398eLL74of8+xrJ2cnBwkJCTA2dmZr8l7ZeizxOkfW7ZsESYmJmLjxo3i1KlTYvz48cLGxkbv6oKHTXZ2tjh+/Lg4fvy4ACCWL18ujh8/Li5duiSEKLvM38bGRnz//ffijz/+EIMGDar0Mv+OHTuK2NhY8dtvvwlvb2+9y/wzMjKEo6OjGDFihDh58qTYsmWLMDMze6Au83/99deFtbW12Lt3r97lwHl5eXKf1157TTRv3lzs3r1bHDlyRAQGBorAwEB5evnlwH379hVxcXEiMjJSNG3atNLLgWfMmCFOnz4t1qxZ80BdDvz222+Lffv2icTERPHHH3+It99+W0iSJHbt2iWE4Bjei9uvYhOCY1ld06ZNE3v37hWJiYniwIEDIjg4WNjb24v09HQhBMfxXjAgNTCrV68WzZs3FxqNRnTp0kX8/vvvhi7JoPbs2SMAVHiMGjVKCFF2qf/cuXOFo6OjMDExEY8//riIj4/XW8aNGzfE8OHDhYWFhbCyshJjxowR2dnZen1OnDghgoKChImJiWjWrJlYtGhRfW1ivahsDAGIDRs2yH3y8/PFhAkThK2trTAzMxPPPPOMuHr1qt5yLl68KPr37y9MTU2Fvb29mDZtmiguLtbrs2fPHuHr6ys0Go145JFH9NbR2L388svC3d1daDQa0bRpU/H444/L4UgIjuG9UAYkjmX1DB06VDg7OwuNRiOaNWsmhg4dKs6fPy9P5zjWniSEEIbZd0VERETUMPEcJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiIiIiBQYkIiIiIgUGJCIiIiIFBiQiahQuXrwISZIQFxdn6FJkZ86cQdeuXaHVauHr62vocirVu3dvTJkyxdBlEDU6DEhEVC2jR4+GJElYtGiRXvv27dshSZKBqjKssLAwmJubIz4+vsJnVQFAREQELC0tUVJSIrfl5OTA2NgYvXv31uu7d+9eSJKEhISE+102EVUDAxIRVZtWq8XixYtx69YtQ5dSZ4qKimo9b0JCAoKCguDu7o4mTZpUmN6nTx/k5OTgyJEjctuvv/4KJycnxMbGoqCgQG7fs2cPmjdvDi8vrxrXIYTQC2FEdO8YkIio2oKDg+Hk5ITw8PAq+8yfP7/C4aYVK1bAw8NDfj569GiEhoZi4cKFcHR0hI2NDd577z2UlJRgxowZsLOzg6urKzZs2FBh+WfOnEG3bt2g1WrRvn177Nu3T2/6yZMn0b9/f1hYWMDR0REjRozA9evX5em9e/fGpEmTMGXKFNjb2yMkJKTS7dDpdHjvvffg6uoKExMT+Pr6IjIyUp4uSRKOHj2K9957D5IkYf78+RWW0apVKzg7O2Pv3r1y2969ezFo0CB4enri999/12vv06cPAKCwsBBvvvkmHBwcoNVqERQUhMOHD+v1lSQJO3fuhJ+fH0xMTPDbb78hNzcXI0eOhIWFBZydnbFs2bIKNa1duxbe3t7QarVwdHTEc889V+n2Ez3sGJCIqNrUajUWLlyI1atX4/Lly/e0rN27dyMlJQX79+/H8uXLERYWhieffBK2traIjY3Fa6+9hldffbXCembMmIFp06bh+PHjCAwMxFNPPYUbN24AADIyMvDYY4+hY8eOOHLkCCIjI5GWloYhQ4boLeOLL76ARqPBgQMHEBERUWl9K1euxLJly/Dhhx/ijz/+QEhICJ5++mmcO3cOAHD16lW0a9cO06ZNw9WrVzF9+vRKl9OnTx/s2bNHfr5nzx707t0bvXr1ktvz8/MRGxsrB6SZM2fim2++wRdffIFjx46hRYsWCAkJwc2bN/WW/fbbb2PRokU4ffo0Hn30UcyYMQP79u3D999/j127dmHv3r04duyY3P/IkSN488038d577yE+Ph6RkZHo2bPnXX9WRA8lA39YLhE1EqNGjRKDBg0SQgjRtWtX8fLLLwshhPjuu+/E7W8lYWFhwsfHR2/ejz76SLi7u+sty93dXZSWlsptrVq1Ej169JCfl5SUCHNzc/H1118LIYRITEwUAMSiRYvkPsXFxcLV1VUsXrxYCCHEggULRN++ffXWnZycLACI+Ph4IUTZp8Z37Njxrtvr4uIiPvjgA722zp07iwkTJsjPfXx8RFhY2B2X8+mnnwpzc3NRXFwssrKyhJGRkUhPTxebN28WPXv2FEIIER0dLQCIS5cuiZycHGFsbCy++uoreRlFRUXCxcVFLFmyRAhR9snqAMT27dvlPtnZ2UKj0Yj//ve/ctuNGzeEqampmDx5shBCiG+++UZYWVmJrKysu24/0cOOe5CIqMYWL16ML774AqdPn671Mtq1aweV6p+3IEdHR3To0EF+rlar0aRJE6Snp+vNFxgYKH9vZGQEf39/uY4TJ05gz549sLCwkB+tW7cGAL2Tn/38/O5YW1ZWFlJSUtC9e3e99u7du9d4m3v37o3c3FwcPnwYv/76K1q2bImmTZuiV69e8nlIe/fuxSOPPILmzZsjISEBxcXFeus2NjZGly5dKqzb399f/j4hIQFFRUUICAiQ2+zs7NCqVSv5+RNPPAF3d3c88sgjGDFiBL766ivk5eXVaHuIHhYMSERUYz179kRISAhmzZpVYZpKpYIQQq+tuLi4Qj9jY2O955IkVdqm0+mqXVdOTg6eeuopxMXF6T3OnTundyjJ3Ny82su8Vy1atICrqyv27NmDPXv2oFevXgAAFxcXuLm54eDBg9izZw8ee+yxGi+7ptthaWmJY8eO4euvv4azszPmzZsHHx8fZGRk1HjdRA86BiQiqpVFixbhf//7H2JiYvTamzZtitTUVL2QVJf3Lrr9xOaSkhIcPXoUbdq0AQB06tQJf/31Fzw8PNCiRQu9R03ChJWVFVxcXHDgwAG99gMHDqBt27Y1rrlPnz7Yu3cv9u7dq3d5f8+ePbFz504cOnRIPv/Iy8tLPj+qXHFxMQ4fPnzHdXt5ecHY2BixsbFy261bt3D27Fm9fkZGRggODsaSJUvwxx9/4OLFi9i9e3eNt4noQWdk6AKIqHHq0KEDXnzxRaxatUqvvXfv3rh27RqWLFmC5557DpGRkdi5cyesrKzqZL1r1qyBt7c32rRpg48++gi3bt3Cyy+/DACYOHEiPv30UwwfPhwzZ86EnZ0dzp8/jy1btuCzzz6DWq2u9npmzJiBsLAweHl5wdfXFxs2bEBcXBy++uqrGtfcp08fTJw4EcXFxfIeJADo1asXJk2ahKKiIjkgmZub4/XXX5ev5mvevDmWLFmCvLw8jB07tsp1WFhYYOzYsZgxYwaaNGkCBwcHvPPOO3qHMXfs2IELFy6gZ8+esLW1xU8//QSdTqd3GI6IyjAgEVGtvffee9i6dateW5s2bbB27VosXLgQCxYswODBgzF9+nSsX7++Tta5aNEiLFq0CHFxcWjRogV++OEH2NvbA4C81+ett95C3759UVhYCHd3d/Tr108vKFTHm2++iczMTEybNg3p6elo27YtfvjhB3h7e9e45j59+iA/Px+tW7eGo6Oj3N6rVy9kZ2fLtwO4fRt1Oh1GjBiB7Oxs+Pv74+eff4atre0d17N06VL5MKOlpSWmTZuGzMxMebqNjQ2+/fZbzJ8/HwUFBfD29sbXX3+Ndu3a1XibiB50klCeLEBERET0kOM5SEREREQKDEhERERECgxIRERERAoMSEREREQKDEhERERECgxIRERERAoMSEREREQKDEhERERECgxIRERERAoMSEREREQKDEhERERECgxIRERERAr/D1Gq8IO18UouAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"Description\"], df[\"Social\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "text_lengths = [len(t.split()) for t in train_texts]\n",
    "ax = sns.histplot(data=text_lengths, kde=True, stat=\"density\")\n",
    "ax.set_title(\"Texts Length Distribution (Number of Words)\")\n",
    "ax.set_xlabel(\"Number of Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Subjective', 'Gender', 'Jargon', 'Social']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# credit: https://github.com/NielsRogge/Transformers-Tutorials\n",
    "labels = [label for label in dataset[\"train\"].features.keys() if label not in [\"ObjectID\", \"Description\"]]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# credit: https://github.com/NielsRogge/Transformers-Tutorials\n",
    "def preprocess_data(data):\n",
    "\n",
    "\t# save the given batch of descs\n",
    "\tdescs = data[\"Description\"]\n",
    "\n",
    "\t# encode them using bert tokenizer\n",
    "\tencoding = tokenizer(descs, padding=True, truncation=True, max_length=512)#.to(\"mps\")\n",
    "\n",
    "\t# create numpy array (no need to convert T/F to 0/1 since we annotated that way)\n",
    "\t# MATRIX FORMAT:\n",
    "\t# |---------------------------------\n",
    "\t# | bias   | bias1 bias2 bias3 bias4\n",
    "\t# |--------+------------------------\n",
    "\t# | desc0  |   1     0     1     0\n",
    "\t# | desc1  |   0     1     0     1\n",
    "\t# | desc2  |   0     1     0     0\n",
    "\t# | ...    |  ...   ...   ...   ...\n",
    "\t# \n",
    "\t# Convert integers to float and data to an NDarray\n",
    "\tsubjective = np.array(data[\"Subjective\"], dtype=float)\n",
    "\tgender = np.array(data[\"Gender\"], dtype=float)\n",
    "\tjargon = np.array(data[\"Jargon\"], dtype=float)\n",
    "\tsocial = np.array(data[\"Social\"], dtype=float)\n",
    "\t# Stack the arrays column-wise to form a 2D array (matrix)\n",
    "\tlabels_matrix = np.stack((subjective, gender, jargon, social), axis=1)\n",
    "\n",
    "\t\n",
    "\t# # Credit ChatGPT\n",
    "\t# # Validate the data stacking by comparing 3 random indices\n",
    "\t# import random\n",
    "\t# for _ in range(3):\n",
    "\t# \tidx = random.randint(0, len(subjective) - 1)\n",
    "\t# \tdataset_labels = [data[\"Subjective\"][idx], data[\"Gender\"][idx], data[\"Jargon\"][idx], data[\"Social\"][idx]]\n",
    "\t# \tmatrix_labels = labels_matrix[idx].tolist()\n",
    "\t# \tassert dataset_labels == matrix_labels, f\"Mismatch at index {idx}: {dataset_labels} != {matrix_labels}\"\n",
    "\t# \tprint(f\"Index {idx} matches: {dataset_labels}\")\n",
    "\n",
    "\n",
    "\t# FORMAT OF var encoding of type BatchEncoding (the length of the vals of each key \n",
    "\t# equal the num of descs/objects in given batch):\n",
    "\t# input_ids: [101, 1030, 4748, 7229, 1035, ...], ...\n",
    "\t# token_type_ids: [0, 0, 0, 0, 0, ...], ...\n",
    "\t# attention_mask: [1, 1, 1, 1, 1, ...], ...\n",
    "\t# labels: [1.0, 1.0, 0.0, 0.0], ...\n",
    "\tencoding[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "\treturn encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1280/1280 [00:05<00:00, 241.67 examples/s]\n",
      "Map: 100%|██████████| 160/160 [00:00<00:00, 228.95 examples/s]\n",
      "Map: 100%|██████████| 161/161 [00:00<00:00, 272.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] pendant in the form of a bird? no provenance information in files. date range comes from memo in accession lot that says witten collection was built between 1960 and 1985. location verified, inventory 2001 location verified, inventory 2003. location verified, inventory 9, fall 2004. october 1992 descriptor moved. describes buildings having engaged columns or pilasters along the sides and rear and freestanding columns along the front. doesn't look real. not worth looking at under magnification! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[1.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# see example\n",
    "print(tokenizer.decode(encoded_dataset[\"train\"][0][\"input_ids\"]))\n",
    "print(encoded_dataset[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset a standard torch dataset by converting to tensors (and more?)\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)#.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add Warmup?\n",
    "Experiment with learning rate\n",
    "experiment with batch size\n",
    "experient with gradient_accumulation_steps\n",
    "another metric?\n",
    "Choose another optimizer: RMSprop, SGD...\n",
    "Increase the learning rate by default and then use the callback ReduceLROnPlateau\n",
    "\"\"\"\n",
    "# use keras instead of huggung face to make it easier to work with messing with layers\n",
    "# remove entries greater than 512 words to remove noise\n",
    "# enchance data by repeatung key terms\n",
    "# cut 512 from middle of the dataset\n",
    "# try giving it only the labels with 5 word context\n",
    "# try doing subtext technique to give it 1000 words\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "lr = 2e-5\n",
    "metric_name = \"f1\"\n",
    "decay = 0.01\n",
    "\n",
    "model_path = \"/mnt/d/Skull/coding/model/\"\n",
    "tokenizer_path = \"./tokenizer\"\n",
    "logs_path = \"./logs\"\n",
    "\n",
    "with open('../../../hg_token.txt', 'r') as file:\n",
    "\thg_token = file.read()\n",
    "\n",
    "\n",
    "# args for training the model\n",
    "# save the model every epoch and choose the best performing epoch as the final version of the model\n",
    "args = TrainingArguments(\n",
    "\teval_strategy = \"epoch\",\n",
    "\tsave_strategy = \"epoch\",\n",
    "    # save_total_limit = 5,\n",
    "\tlogging_strategy = \"epoch\",\n",
    "\tlearning_rate = lr,\n",
    "\tper_device_train_batch_size = batch_size,\n",
    "\tper_device_eval_batch_size = batch_size,\n",
    "\tnum_train_epochs = num_epochs,\n",
    "\tweight_decay = decay,\n",
    "\tload_best_model_at_end = True,\n",
    "\tmetric_for_best_model = metric_name,\n",
    "\tlogging_dir = logs_path,\n",
    "\toutput_dir = model_path,\n",
    "    warmup_steps=100,\n",
    "\t# use_mps_device = True,\n",
    "\tuse_cpu = False,\n",
    "\tlogging_steps = 1,\n",
    "\t# gradient_accumulation_steps=2,\n",
    "\thub_token = hg_token,\n",
    "\thub_model_id = \"raasikhk/carlos_bert_v2\",\n",
    "\tpush_to_hub=True,\n",
    ")\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "from numpy import ndarray\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "def get_next_image_number(directory: str) -> int:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        return 1\n",
    "    images = glob.glob(os.path.join(directory, '*.png'))\n",
    "    if not images:\n",
    "        return 1\n",
    "    numbers = [int(os.path.basename(image).split('_')[0]) for image in images]\n",
    "    return max(numbers) + 1\n",
    "\n",
    "def plot_confusion_matrix(cm, save_path, title):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(cm))\n",
    "    plt.xticks(tick_marks, tick_marks, rotation=45)\n",
    "    plt.yticks(tick_marks, tick_marks)\n",
    "    \n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm[i])):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def my_accuracy_score(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[int, int, int, int]:\n",
    "    directory = 'cm'\n",
    "    image_number = get_next_image_number(directory)\n",
    "    \n",
    "    labels = [\"Subjective\", \"Gender\", \"Jargon\", \"Social\"]\n",
    "    true_pos_list, false_pos_list, true_neg_list, false_neg_list = [], [], [], []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        save_path = os.path.join(directory, f'{image_number}_{label}.png')\n",
    "        \n",
    "        # Calculate confusion matrix for the current label\n",
    "        cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(cm, save_path, f'Confusion Matrix - {label}')\n",
    "        \n",
    "        # Calculate true positives, false positives, true negatives, false negatives\n",
    "        true_pos = np.sum((y_true[:, i] == 1) & (y_pred[:, i] == 1))\n",
    "        false_pos = np.sum((y_true[:, i] == 0) & (y_pred[:, i] == 1))\n",
    "        true_neg = np.sum((y_true[:, i] == 0) & (y_pred[:, i] == 0))\n",
    "        false_neg = np.sum((y_true[:, i] == 1) & (y_pred[:, i] == 0))\n",
    "        \n",
    "        true_pos_list.append(true_pos)\n",
    "        false_pos_list.append(false_pos)\n",
    "        true_neg_list.append(true_neg)\n",
    "        false_neg_list.append(false_neg)\n",
    "    \n",
    "    return (\n",
    "        sum(true_pos_list), sum(false_pos_list), \n",
    "        sum(true_neg_list), sum(false_neg_list)\n",
    "    )\n",
    "\n",
    "\n",
    "def partial_accuracy_score(y_true: ndarray, y_pred: ndarray):\n",
    "\tnum_objects = len(y_true)\n",
    "\tnum_labels = len(y_true)*4\n",
    "\tcorrect_predictions = 0\n",
    "\t\n",
    "\tfor i in range(num_objects):\n",
    "\t\tfor j in range(len(y_true[i])):\n",
    "\t\t\tif y_true[i][j] == y_pred[i][j]:\n",
    "\t\t\t\tcorrect_predictions += 1\n",
    "\t\n",
    "\taccuracy = correct_predictions / num_labels\n",
    "\treturn accuracy\n",
    "\n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "\t# first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "\tsigmoid = torch.nn.Sigmoid()\n",
    "\tprobs = sigmoid(torch.Tensor(predictions))\n",
    "\t# next, use threshold to turn them into integer predictions\n",
    "\ty_pred = np.zeros(probs.shape)\n",
    "\ty_pred[np.where(probs >= threshold)] = 1\n",
    "\t# finally, compute metrics\n",
    "\ty_true = labels\n",
    "\tf1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "\troc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "\taccuracy = accuracy_score(y_true, y_pred)\n",
    "\tmyacc = partial_accuracy_score(y_true, y_pred)\n",
    "\ttrue_pos, false_pos, true_neg, false_neg = my_accuracy_score(y_true, y_pred)\n",
    "\t# return as dictionary\n",
    "\tmetrics = {'f1': f1_micro_average,\n",
    "\t\t\t\t'roc_auc': roc_auc,\n",
    "\t\t\t\t'exact_match_acc': accuracy,\n",
    "\t\t\t\t\"partial_acc\": myacc,\n",
    "\t\t\t\t'true_pos': true_pos,\n",
    "        \t\t'true_neg': true_neg,\n",
    "        \t\t'false_neg': false_neg,\n",
    "\t\t\t\t'false_pos': false_pos}\n",
    "\treturn metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "\tpreds = p.predictions[0] if isinstance(p.predictions, \n",
    "\t\t\ttuple) else p.predictions\n",
    "\tresult = multi_label_metrics(\n",
    "\t\tpredictions=preds, \n",
    "\t\tlabels=p.label_ids)\n",
    "\treturn result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "\tmodel,\n",
    "\targs,\n",
    "\ttrain_dataset=encoded_dataset[\"train\"],\n",
    "\teval_dataset=encoded_dataset[\"validation\"],\n",
    "\ttokenizer=tokenizer,\n",
    "\tcompute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16000' max='16000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16000/16000 2:42:51, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Exact Match Acc</th>\n",
       "      <th>Partial Acc</th>\n",
       "      <th>True Pos</th>\n",
       "      <th>True Neg</th>\n",
       "      <th>False Neg</th>\n",
       "      <th>False Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.428400</td>\n",
       "      <td>0.301900</td>\n",
       "      <td>0.577540</td>\n",
       "      <td>0.737861</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.877329</td>\n",
       "      <td>54</td>\n",
       "      <td>511</td>\n",
       "      <td>47</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.283439</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.705277</td>\n",
       "      <td>0.596273</td>\n",
       "      <td>0.883540</td>\n",
       "      <td>45</td>\n",
       "      <td>524</td>\n",
       "      <td>56</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.294516</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>0.758930</td>\n",
       "      <td>0.577640</td>\n",
       "      <td>0.878882</td>\n",
       "      <td>59</td>\n",
       "      <td>507</td>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.320963</td>\n",
       "      <td>0.528090</td>\n",
       "      <td>0.705049</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>47</td>\n",
       "      <td>513</td>\n",
       "      <td>54</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.547264</td>\n",
       "      <td>0.730841</td>\n",
       "      <td>0.515528</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>55</td>\n",
       "      <td>498</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.153400</td>\n",
       "      <td>0.371906</td>\n",
       "      <td>0.554348</td>\n",
       "      <td>0.723009</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.872671</td>\n",
       "      <td>51</td>\n",
       "      <td>511</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.389813</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.736366</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>55</td>\n",
       "      <td>504</td>\n",
       "      <td>46</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.425688</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.750296</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>58</td>\n",
       "      <td>503</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.438775</td>\n",
       "      <td>0.579186</td>\n",
       "      <td>0.765266</td>\n",
       "      <td>0.527950</td>\n",
       "      <td>0.855590</td>\n",
       "      <td>64</td>\n",
       "      <td>487</td>\n",
       "      <td>37</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.442058</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.491115</td>\n",
       "      <td>0.554974</td>\n",
       "      <td>0.728306</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>53</td>\n",
       "      <td>506</td>\n",
       "      <td>48</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.517753</td>\n",
       "      <td>0.604444</td>\n",
       "      <td>0.785068</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.861801</td>\n",
       "      <td>68</td>\n",
       "      <td>487</td>\n",
       "      <td>33</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.553837</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.768375</td>\n",
       "      <td>0.509317</td>\n",
       "      <td>0.854037</td>\n",
       "      <td>65</td>\n",
       "      <td>485</td>\n",
       "      <td>36</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.562667</td>\n",
       "      <td>0.606635</td>\n",
       "      <td>0.774474</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>64</td>\n",
       "      <td>497</td>\n",
       "      <td>37</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.537216</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.773554</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>64</td>\n",
       "      <td>496</td>\n",
       "      <td>37</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.602558</td>\n",
       "      <td>0.574163</td>\n",
       "      <td>0.752831</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.861801</td>\n",
       "      <td>60</td>\n",
       "      <td>495</td>\n",
       "      <td>41</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.584517</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.745692</td>\n",
       "      <td>0.527950</td>\n",
       "      <td>0.863354</td>\n",
       "      <td>58</td>\n",
       "      <td>498</td>\n",
       "      <td>43</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.575784</td>\n",
       "      <td>0.593301</td>\n",
       "      <td>0.764573</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>62</td>\n",
       "      <td>497</td>\n",
       "      <td>39</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.580219</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.785871</td>\n",
       "      <td>0.590062</td>\n",
       "      <td>0.883540</td>\n",
       "      <td>65</td>\n",
       "      <td>504</td>\n",
       "      <td>36</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.662526</td>\n",
       "      <td>0.561224</td>\n",
       "      <td>0.735445</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>55</td>\n",
       "      <td>503</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.660996</td>\n",
       "      <td>0.595349</td>\n",
       "      <td>0.770791</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>64</td>\n",
       "      <td>493</td>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.666699</td>\n",
       "      <td>0.561224</td>\n",
       "      <td>0.735445</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>55</td>\n",
       "      <td>503</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.685525</td>\n",
       "      <td>0.556098</td>\n",
       "      <td>0.738900</td>\n",
       "      <td>0.527950</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>57</td>\n",
       "      <td>496</td>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.675748</td>\n",
       "      <td>0.595349</td>\n",
       "      <td>0.770791</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>64</td>\n",
       "      <td>493</td>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.695459</td>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.771712</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>64</td>\n",
       "      <td>494</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.737124</td>\n",
       "      <td>0.566210</td>\n",
       "      <td>0.755365</td>\n",
       "      <td>0.509317</td>\n",
       "      <td>0.852484</td>\n",
       "      <td>62</td>\n",
       "      <td>487</td>\n",
       "      <td>39</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.715960</td>\n",
       "      <td>0.596330</td>\n",
       "      <td>0.773900</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.863354</td>\n",
       "      <td>65</td>\n",
       "      <td>491</td>\n",
       "      <td>36</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.731244</td>\n",
       "      <td>0.579439</td>\n",
       "      <td>0.759969</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.860248</td>\n",
       "      <td>62</td>\n",
       "      <td>492</td>\n",
       "      <td>39</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.740838</td>\n",
       "      <td>0.582524</td>\n",
       "      <td>0.755593</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>60</td>\n",
       "      <td>498</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.737485</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.750068</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>60</td>\n",
       "      <td>492</td>\n",
       "      <td>41</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.768076</td>\n",
       "      <td>0.575472</td>\n",
       "      <td>0.755940</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.860248</td>\n",
       "      <td>61</td>\n",
       "      <td>493</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.764400</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.751910</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.860248</td>\n",
       "      <td>60</td>\n",
       "      <td>494</td>\n",
       "      <td>41</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.761523</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.757435</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>60</td>\n",
       "      <td>500</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.782515</td>\n",
       "      <td>0.588785</td>\n",
       "      <td>0.765841</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.863354</td>\n",
       "      <td>63</td>\n",
       "      <td>493</td>\n",
       "      <td>38</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.799107</td>\n",
       "      <td>0.568720</td>\n",
       "      <td>0.750989</td>\n",
       "      <td>0.527950</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>60</td>\n",
       "      <td>493</td>\n",
       "      <td>41</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.814241</td>\n",
       "      <td>0.586538</td>\n",
       "      <td>0.759623</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>61</td>\n",
       "      <td>497</td>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.825904</td>\n",
       "      <td>0.527919</td>\n",
       "      <td>0.716910</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.855590</td>\n",
       "      <td>52</td>\n",
       "      <td>499</td>\n",
       "      <td>49</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.790843</td>\n",
       "      <td>0.563380</td>\n",
       "      <td>0.749148</td>\n",
       "      <td>0.515528</td>\n",
       "      <td>0.855590</td>\n",
       "      <td>60</td>\n",
       "      <td>491</td>\n",
       "      <td>41</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.780152</td>\n",
       "      <td>0.597156</td>\n",
       "      <td>0.768603</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>63</td>\n",
       "      <td>496</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.819879</td>\n",
       "      <td>0.520408</td>\n",
       "      <td>0.711960</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.854037</td>\n",
       "      <td>51</td>\n",
       "      <td>499</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.824659</td>\n",
       "      <td>0.557078</td>\n",
       "      <td>0.749494</td>\n",
       "      <td>0.509317</td>\n",
       "      <td>0.849379</td>\n",
       "      <td>61</td>\n",
       "      <td>486</td>\n",
       "      <td>40</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.780107</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.754672</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>60</td>\n",
       "      <td>497</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.799289</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.763653</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>62</td>\n",
       "      <td>496</td>\n",
       "      <td>39</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.811441</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.743504</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>57</td>\n",
       "      <td>501</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.816334</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.737633</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.863354</td>\n",
       "      <td>56</td>\n",
       "      <td>500</td>\n",
       "      <td>45</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.796457</td>\n",
       "      <td>0.600939</td>\n",
       "      <td>0.772633</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>64</td>\n",
       "      <td>495</td>\n",
       "      <td>37</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.808027</td>\n",
       "      <td>0.591133</td>\n",
       "      <td>0.758356</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>60</td>\n",
       "      <td>501</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.812888</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.756514</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>60</td>\n",
       "      <td>499</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.806781</td>\n",
       "      <td>0.574163</td>\n",
       "      <td>0.752831</td>\n",
       "      <td>0.527950</td>\n",
       "      <td>0.861801</td>\n",
       "      <td>60</td>\n",
       "      <td>495</td>\n",
       "      <td>41</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.760150</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.760197</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.874224</td>\n",
       "      <td>60</td>\n",
       "      <td>503</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.797031</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.757435</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>60</td>\n",
       "      <td>500</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.772251</td>\n",
       "      <td>0.619469</td>\n",
       "      <td>0.795890</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>70</td>\n",
       "      <td>488</td>\n",
       "      <td>31</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.810753</td>\n",
       "      <td>0.583732</td>\n",
       "      <td>0.758702</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>61</td>\n",
       "      <td>496</td>\n",
       "      <td>40</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.806970</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.756514</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>60</td>\n",
       "      <td>499</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.843356</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.754672</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>60</td>\n",
       "      <td>497</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.843511</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.755247</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.872671</td>\n",
       "      <td>59</td>\n",
       "      <td>503</td>\n",
       "      <td>42</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.893957</td>\n",
       "      <td>0.593607</td>\n",
       "      <td>0.772979</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.861801</td>\n",
       "      <td>65</td>\n",
       "      <td>490</td>\n",
       "      <td>36</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.837287</td>\n",
       "      <td>0.582011</td>\n",
       "      <td>0.741890</td>\n",
       "      <td>0.577640</td>\n",
       "      <td>0.877329</td>\n",
       "      <td>55</td>\n",
       "      <td>510</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.857965</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.750068</td>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>60</td>\n",
       "      <td>492</td>\n",
       "      <td>41</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.857035</td>\n",
       "      <td>0.594059</td>\n",
       "      <td>0.759276</td>\n",
       "      <td>0.583851</td>\n",
       "      <td>0.872671</td>\n",
       "      <td>60</td>\n",
       "      <td>502</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.866605</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.746613</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>58</td>\n",
       "      <td>499</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.865623</td>\n",
       "      <td>0.575916</td>\n",
       "      <td>0.740049</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.874224</td>\n",
       "      <td>55</td>\n",
       "      <td>508</td>\n",
       "      <td>46</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.863763</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.741316</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>56</td>\n",
       "      <td>504</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.859152</td>\n",
       "      <td>0.575610</td>\n",
       "      <td>0.750643</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>59</td>\n",
       "      <td>498</td>\n",
       "      <td>42</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.851033</td>\n",
       "      <td>0.575916</td>\n",
       "      <td>0.740049</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.874224</td>\n",
       "      <td>55</td>\n",
       "      <td>508</td>\n",
       "      <td>46</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.857434</td>\n",
       "      <td>0.574359</td>\n",
       "      <td>0.742237</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>56</td>\n",
       "      <td>505</td>\n",
       "      <td>45</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.865957</td>\n",
       "      <td>0.579710</td>\n",
       "      <td>0.754672</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>60</td>\n",
       "      <td>497</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.861186</td>\n",
       "      <td>0.592965</td>\n",
       "      <td>0.756168</td>\n",
       "      <td>0.577640</td>\n",
       "      <td>0.874224</td>\n",
       "      <td>59</td>\n",
       "      <td>504</td>\n",
       "      <td>42</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.864054</td>\n",
       "      <td>0.584158</td>\n",
       "      <td>0.753405</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>59</td>\n",
       "      <td>501</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.868980</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.745346</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>57</td>\n",
       "      <td>503</td>\n",
       "      <td>44</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.870294</td>\n",
       "      <td>0.578680</td>\n",
       "      <td>0.746267</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>57</td>\n",
       "      <td>504</td>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.868369</td>\n",
       "      <td>0.572864</td>\n",
       "      <td>0.744425</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>57</td>\n",
       "      <td>502</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.871072</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.745346</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>57</td>\n",
       "      <td>503</td>\n",
       "      <td>44</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.872705</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.747534</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>58</td>\n",
       "      <td>500</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.869562</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.750296</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>58</td>\n",
       "      <td>503</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.876670</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.750296</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>58</td>\n",
       "      <td>503</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.877828</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.895867</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.756514</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>60</td>\n",
       "      <td>499</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.884201</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.751564</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>59</td>\n",
       "      <td>499</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.886932</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.885420</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.885376</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.746613</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>58</td>\n",
       "      <td>499</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.885253</td>\n",
       "      <td>0.578680</td>\n",
       "      <td>0.746267</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>57</td>\n",
       "      <td>504</td>\n",
       "      <td>44</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.891949</td>\n",
       "      <td>0.562814</td>\n",
       "      <td>0.738554</td>\n",
       "      <td>0.540373</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>56</td>\n",
       "      <td>501</td>\n",
       "      <td>45</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.889668</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.890481</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.746613</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.864907</td>\n",
       "      <td>58</td>\n",
       "      <td>499</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.890098</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.888327</td>\n",
       "      <td>0.582915</td>\n",
       "      <td>0.750296</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>58</td>\n",
       "      <td>503</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.883289</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.757435</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>60</td>\n",
       "      <td>500</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.885491</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.743504</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>57</td>\n",
       "      <td>501</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.889501</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.747534</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>58</td>\n",
       "      <td>500</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.581281</td>\n",
       "      <td>0.752484</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>59</td>\n",
       "      <td>500</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.896721</td>\n",
       "      <td>0.591133</td>\n",
       "      <td>0.758356</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.871118</td>\n",
       "      <td>60</td>\n",
       "      <td>501</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.896230</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.757435</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>60</td>\n",
       "      <td>500</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.897151</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.897657</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.897338</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.897559</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.897187</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.897613</td>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.748455</td>\n",
       "      <td>0.552795</td>\n",
       "      <td>0.868012</td>\n",
       "      <td>58</td>\n",
       "      <td>501</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16000, training_loss=0.028679579094052315, metrics={'train_runtime': 9773.3715, 'train_samples_per_second': 13.097, 'train_steps_per_second': 1.637, 'total_flos': 3.3678819852288e+16, 'train_loss': 0.028679579094052315, 'epoch': 100.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainOutput(global_step=1200, training_loss=0.05973712073231582, metrics={'train_runtime': 1398.0176, 'train_samples_per_second': 13.734, 'train_steps_per_second': 0.858, 'total_flos': 5051822977843200.0, 'train_loss': 0.05973712073231582, 'epoch': 15.0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5802189707756042,\n",
       " 'eval_f1': 0.6341463414634146,\n",
       " 'eval_roc_auc': 0.7858705760078771,\n",
       " 'eval_exact_match_acc': 0.5900621118012422,\n",
       " 'eval_partial_acc': 0.8835403726708074,\n",
       " 'eval_true_pos': 65,\n",
       " 'eval_true_neg': 504,\n",
       " 'eval_false_neg': 36,\n",
       " 'eval_false_pos': 39,\n",
       " 'eval_runtime': 3.6007,\n",
       " 'eval_samples_per_second': 44.714,\n",
       " 'eval_steps_per_second': 5.832,\n",
       " 'epoch': 100.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'eval_loss': 0.324933260679245,\n",
    " 'eval_f1': 0.7070707070707071,\n",
    " 'eval_roc_auc': 0.8216727750123078,\n",
    " 'eval_accuracy': 0.7018633540372671,\n",
    " 'eval_runtime': 3.7853,\n",
    " 'eval_samples_per_second': 42.533,\n",
    " 'eval_steps_per_second': 2.906,\n",
    " 'epoch': 15.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.65.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/raasikhk/Coding/ai.xperience/carlos-artifact-tagging-bias/myenv/lib/python3.11/site-packages (from tensorboard) (1.26.4)\n",
      "Collecting protobuf!=4.24.0,<5.0.0,>=3.19.6 (from tensorboard)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/raasikhk/Coding/ai.xperience/carlos-artifact-tagging-bias/myenv/lib/python3.11/site-packages (from tensorboard) (65.5.0)\n",
      "Requirement already satisfied: six>1.9 in /home/raasikhk/Coding/ai.xperience/carlos-artifact-tagging-bias/myenv/lib/python3.11/site-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/raasikhk/Coding/ai.xperience/carlos-artifact-tagging-bias/myenv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Using cached tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading grpcio-1.65.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.65.1 markdown-3.6 protobuf-4.25.3 tensorboard-2.17.0 tensorboard-data-server-0.7.2 werkzeug-3.0.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "TensorFlow installation not found - running with reduced feature set.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721830932.526459   75105 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n",
      "I0000 00:00:1721830932.529415   75163 subchannel.cc:806] subchannel 0x5581862e6a50 {address=ipv6:%5B::1%5D:35793, args={grpc.client_channel_factory=0x5581862a1bb0, grpc.default_authority=localhost:35793, grpc.internal.channel_credentials=0x5581862f07a0, grpc.internal.client_channel_call_destination=0x7f83373a73d0, grpc.internal.event_engine=0x5581860a2260, grpc.internal.security_connector=0x558185ef7770, grpc.internal.subchannel_pool=0x558185d6a220, grpc.max_receive_message_length=268435456, grpc.primary_user_agent=grpc-python/1.65.1, grpc.resource_quota=0x558185a85cd0, grpc.server_uri=dns:///localhost:35793}}: connect failed (UNKNOWN:Failed to connect to remote host: connect: Connection refused (111) {created_time:\"2024-07-24T10:22:12.529126573-04:00\"}), backing off for 1000 ms\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.17.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# view logs (only needed for analysis)\n",
    "# !pip install tensorboard\n",
    "!tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"model\")\n",
    "# tokenizer.save_pretrained(\"tokenizer\")\n",
    "# tokenizer = transformers.BertTokenizer.from_pretrained(\"tokenizer\")\n",
    "# model = transformers.BertForSequenceClassification.from_pretrained(\"model/checkpoint-1200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0007, 0.0031, 0.0750, 0.0016], grad_fn=<SigmoidBackward0>)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# test a description\n",
    "text = \"December 1992 lead-in term added. January 1991 alternate term added. Object fumigated in Orkin's Piedmont vault with Vikane in 1994\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "\n",
    "outputs = trainer.model(**encoding)\n",
    "\n",
    "logits = outputs.logits\n",
    "logits.shape\n",
    "\n",
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "print(probs)\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTION**\n",
    "\n",
    "Run the first code block only if you have the model folder and have NOT done training above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The artifact was created in the 19th century and is considered highly valuable.\n",
      "Predictions: [{'label': 'Subjective', 'score': 0.09203276038169861}, {'label': 'Jargon', 'score': 0.026056140661239624}, {'label': 'Social', 'score': 0.009623771533370018}, {'label': 'Gender', 'score': 0.00038654549280181527}]\n",
      "\n",
      "Text: This piece shows signs of heavy wear and might not be authentic.\n",
      "Predictions: [{'label': 'Subjective', 'score': 0.8277921676635742}, {'label': 'Jargon', 'score': 0.043133534491062164}, {'label': 'Social', 'score': 0.0035685293842107058}, {'label': 'Gender', 'score': 0.0008079292019829154}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST THE MODEL\n",
    "# IF YOU DONT WANT TO TRAIN LOAD MODEL FROM HUGGINGFACE\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "if tokenizer == None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"raasikhk/carlos_bert_v1\")\n",
    "if model == None:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"raasikhk/carlos_bert_v1\")\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=None, truncation=True, padding=True, device=device)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"The artifact was created in the 19th century and is considered highly valuable.\",\n",
    "    \"This piece shows signs of heavy wear and might not be authentic.\"\n",
    "]\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipe(texts)\n",
    "\n",
    "# Print the predictions\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predictions: {predictions[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partial correct score: \n",
      "551 / 644\n",
      "0.8555900621118012\n",
      "\n",
      "All correct score: \n",
      "272 / 644\n",
      "0.422360248447205\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE ACCURACY\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# if tokenizer == None:\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"raasikhk/carlos_bert_v1\")\n",
    "# if model == None:\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\"raasikhk/carlos_bert_v1\")\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=None, truncation=True, padding=True, device=device)\n",
    "\n",
    "# Replace with your test dataset\n",
    "test_descriptions = dataset[\"validation\"][\"Description\"]\n",
    "\n",
    "# Get predictions\n",
    "predictions = pipe(test_descriptions)\n",
    "\n",
    "labels= [\"Subjective\", \"Gender\", \"Jargon\", \"Social\"]\n",
    "\n",
    "print(\"\\nPartial correct score: \")\n",
    "\n",
    "score = 0\n",
    "total = 0\n",
    "for p in predictions:\n",
    "\n",
    "    for i in range(4):\n",
    "        if p[i][\"score\"] >=0.5:\n",
    "            prediction=1\n",
    "        else:\n",
    "            prediction=0\n",
    "        \n",
    "        if dataset[\"validation\"][i][labels[i]] == prediction:\n",
    "            score +=1\n",
    "        total+=1\n",
    "print(f\"{score} / {total}\")\n",
    "print(score/total)\n",
    "\n",
    "print(\"\\nAll correct score: \")\n",
    "\n",
    "score = 0\n",
    "total = 0\n",
    "for p in predictions:\n",
    "    all_c=True\n",
    "    for i in range(4):\n",
    "        if p[i][\"score\"] >=0.5:\n",
    "            prediction=1\n",
    "        else:\n",
    "            prediction=0\n",
    "        \n",
    "        if dataset[\"validation\"][i][labels[i]] != prediction:\n",
    "            all_c = False\n",
    "        total+=1\n",
    "        if (all_c):\n",
    "            score+=1\n",
    "\n",
    "print(f\"{score} / {total}\")\n",
    "print(score/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
